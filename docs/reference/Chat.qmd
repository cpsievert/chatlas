# Chat { #chatlas.Chat }

```python
Chat(self, provider, turns=None)
```

A chat object that can be used to interact with a language model.

A `Chat` is an sequence of sequence of user and assistant
[](`~chatlas.Turn`)s sent to a specific [](`~chatlas.Provider`). A `Chat`
takes care of managing the state associated with the chat; i.e. it records
the messages that you send to the server, and the messages that you receive
back. If you register a tool (i.e. an function that the assistant can call
on your behalf), it also takes care of the tool loop.

You should generally not create this object yourself, but instead call
[](`~chatlas.ChatOpenAI`) or friends instead.

## Attributes

| Name | Description |
| --- | --- |
| [system_prompt](#chatlas.Chat.system_prompt) | Get the system prompt for the chat. |

## Methods

| Name | Description |
| --- | --- |
| [app](#chatlas.Chat.app) | Enter a chat browser to interact with the LLM. |
| [chat](#chatlas.Chat.chat) | Generate a response from the chat. |
| [chat_async](#chatlas.Chat.chat_async) | Generate a response from the chat asynchronously. |
| [console](#chatlas.Chat.console) | Enter a chat console to interact with the LLM. |
| [extract_data](#chatlas.Chat.extract_data) | Extract structured data from the given input. |
| [extract_data_async](#chatlas.Chat.extract_data_async) | Extract structured data from the given input asynchronously. |
| [last_turn](#chatlas.Chat.last_turn) | Get the last turn in the chat with a specific role. |
| [register_tool](#chatlas.Chat.register_tool) | Register a tool with the chat. |
| [submit](#chatlas.Chat.submit) | Submit user input(s) to the chat. |
| [submit_async](#chatlas.Chat.submit_async) | Submit user input(s) to the chat asynchronously. |
| [tokens](#chatlas.Chat.tokens) | Get the tokens for each turn in the chat. |
| [turns](#chatlas.Chat.turns) | Get all the turns (i.e., message contents) in the chat. |

### app { #chatlas.Chat.app }

```python
Chat.app(stream=True, launch_browser=True, port=0, kwargs=None)
```

Enter a chat browser to interact with the LLM.

#### Parameters {.doc-section .doc-section-parameters}

| Name           | Type                                                                                  | Description                                                                          | Default   |
|----------------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-----------|
| stream         | [bool](`bool`)                                                                        | Whether to stream the response (i.e., have the response appear in chunks).           | `True`    |
| launch_browser | [bool](`bool`)                                                                        | Whether to launch a browser window.                                                  | `True`    |
| port           | [int](`int`)                                                                          | The port to run the app on (the default is 0, which will choose a random port).      | `0`       |
| kwargs         | [Optional](`typing.Optional`)\[[ChatRequestArgsT](`chatlas.types.ChatRequestArgsT`)\] | Additional keyword arguments to pass to the method used for requesting the response. | `None`    |

### chat { #chatlas.Chat.chat }

```python
Chat.chat(*args, stream=True, kwargs=None)
```

Generate a response from the chat.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                  | Description                                                                          | Default   |
|--------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-----------|
| args   | [Content](`chatlas.types.Content`) \| [str](`str`)                                    | The user input(s) to generate a response from.                                       | `()`      |
| stream | [bool](`bool`)                                                                        | Whether to stream the response (i.e., have the response appear in chunks).           | `True`    |
| kwargs | [Optional](`typing.Optional`)\[[ChatRequestArgsT](`chatlas.types.ChatRequestArgsT`)\] | Additional keyword arguments to pass to the method used for requesting the response. | `None`    |

### chat_async { #chatlas.Chat.chat_async }

```python
Chat.chat_async(*args, stream=True, kwargs=None)
```

Generate a response from the chat asynchronously.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                  | Description                                                                          | Default   |
|--------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-----------|
| args   | [Content](`chatlas.types.Content`) \| [str](`str`)                                    | The user input(s) to generate a response from.                                       | `()`      |
| stream | [bool](`bool`)                                                                        | Whether to stream the response (i.e., have the response appear in chunks).           | `True`    |
| kwargs | [Optional](`typing.Optional`)\[[ChatRequestArgsT](`chatlas.types.ChatRequestArgsT`)\] | Additional keyword arguments to pass to the method used for requesting the response. | `None`    |

### console { #chatlas.Chat.console }

```python
Chat.console(stream=True, kwargs=None)
```

Enter a chat console to interact with the LLM.

Press Ctrl+C to quit.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                  | Description                                                                         | Default   |
|--------|---------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------|
| stream | [bool](`bool`)                                                                        | Whether to stream the response (i.e., have the response appear in chunks).          | `True`    |
| kwargs | [Optional](`typing.Optional`)\[[ChatRequestArgsT](`chatlas.types.ChatRequestArgsT`)\] | Additional keyword arguments to pass to the method used for requesting the response | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type   | Description   |
|--------|--------|---------------|
|        | None   |               |

### extract_data { #chatlas.Chat.extract_data }

```python
Chat.extract_data(*args, data_model)
```

Extract structured data from the given input.

#### Parameters {.doc-section .doc-section-parameters}

| Name       | Type                                                | Description                                                       | Default    |
|------------|-----------------------------------------------------|-------------------------------------------------------------------|------------|
| args       | [Content](`chatlas.types.Content`) \| [str](`str`)  | The input to extract data from.                                   | `()`       |
| data_model | [type](`type`)\[[BaseModel](`pydantic.BaseModel`)\] | A Pydantic model describing the structure of the data to extract. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                | Description         |
|--------|---------------------|---------------------|
|        | [Any](`typing.Any`) | The extracted data. |

### extract_data_async { #chatlas.Chat.extract_data_async }

```python
Chat.extract_data_async(*args, data_model)
```

Extract structured data from the given input asynchronously.

#### Parameters {.doc-section .doc-section-parameters}

| Name       | Type                                                | Description                                                       | Default    |
|------------|-----------------------------------------------------|-------------------------------------------------------------------|------------|
| args       | [Content](`chatlas.types.Content`) \| [str](`str`)  | The input to extract data from.                                   | `()`       |
| data_model | [type](`type`)\[[BaseModel](`pydantic.BaseModel`)\] | A Pydantic model describing the structure of the data to extract. | _required_ |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                | Description         |
|--------|---------------------|---------------------|
|        | [Any](`typing.Any`) | The extracted data. |

### last_turn { #chatlas.Chat.last_turn }

```python
Chat.last_turn(role='assistant')
```

Get the last turn in the chat with a specific role.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                               | Description                     | Default       |
|--------|--------------------------------------------------------------------|---------------------------------|---------------|
| role   | [Literal](`typing.Literal`)\[\'assistant\', \'user\', \'system\'\] | The role of the turn to return. | `'assistant'` |

### register_tool { #chatlas.Chat.register_tool }

```python
Chat.register_tool(tool)
```

Register a tool with the chat.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                                                                                                                       | Description                                                                        | Default    |
|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|------------|
| tool   | [Callable](`typing.Callable`)\[..., [Any](`typing.Any`)\] \| [Callable](`typing.Callable`)\[..., [Awaitable](`typing.Awaitable`)\[[Any](`typing.Any`)\]\] \| [Tool](`chatlas._tools.Tool`) | The tool to register. This can be a function, an async function, or a Tool object. | _required_ |

### submit { #chatlas.Chat.submit }

```python
Chat.submit(*args, stream=True, kwargs=None)
```

Submit user input(s) to the chat.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                  | Description                                                                          | Default   |
|--------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-----------|
| args   | [Content](`chatlas.types.Content`) \| [str](`str`)                                    | The user input(s) to generate a response from.                                       | `()`      |
| stream | [bool](`bool`)                                                                        | Whether to stream the response (i.e., have the response appear in chunks).           | `True`    |
| kwargs | [Optional](`typing.Optional`)\[[ChatRequestArgsT](`chatlas.types.ChatRequestArgsT`)\] | Additional keyword arguments to pass to the method used for requesting the response. | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                          | Description   |
|--------|-----------------------------------------------|---------------|
|        | A generator that yields the response content. |               |

### submit_async { #chatlas.Chat.submit_async }

```python
Chat.submit_async(*args, stream=True, kwargs=None)
```

Submit user input(s) to the chat asynchronously.

#### Parameters {.doc-section .doc-section-parameters}

| Name   | Type                                                                                  | Description                                                                          | Default   |
|--------|---------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------|-----------|
| args   | [Content](`chatlas.types.Content`) \| [str](`str`)                                    | The user input(s) to generate a response from.                                       | `()`      |
| stream | [bool](`bool`)                                                                        | Whether to stream the response (i.e., have the response appear in chunks).           | `True`    |
| kwargs | [Optional](`typing.Optional`)\[[ChatRequestArgsT](`chatlas.types.ChatRequestArgsT`)\] | Additional keyword arguments to pass to the method used for requesting the response. | `None`    |

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                 | Description   |
|--------|------------------------------------------------------|---------------|
|        | An async generator that yields the response content. |               |

### tokens { #chatlas.Chat.tokens }

```python
Chat.tokens()
```

Get the tokens for each turn in the chat.

#### Returns {.doc-section .doc-section-returns}

| Name   | Type                                                             | Description                                                                             |
|--------|------------------------------------------------------------------|-----------------------------------------------------------------------------------------|
|        | [list](`list`)\[[tuple](`tuple`)\[[int](`int`), [int](`int`)\]\] | A list of tuples, where each tuple contains the start and end token indices for a turn. |

### turns { #chatlas.Chat.turns }

```python
Chat.turns(include_system_prompt=False)
```

Get all the turns (i.e., message contents) in the chat.

#### Parameters {.doc-section .doc-section-parameters}

| Name                  | Type           | Description                                        | Default   |
|-----------------------|----------------|----------------------------------------------------|-----------|
| include_system_prompt | [bool](`bool`) | Whether to include the system prompt in the turns. | `False`   |