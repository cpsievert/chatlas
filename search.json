[
  {
    "objectID": "reference/types.ChatResponseAsync.html",
    "href": "reference/types.ChatResponseAsync.html",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "types.ChatResponseAsync(self, generator)\nChat response (async) object.\nAn object that, when displayed, will simulatenously consume (if not already consumed) and display the response in a streaming fashion.\nThis is useful for interactive use: if the object is displayed, it can be viewed as it is being generated. And, if the object is not displayed, it can act like an iterator that can be consumed by something else.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.\n\n\n\n\n\n\nconsumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndisplay\nDisplay the content in a rich console.\n\n\nget_string\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponseAsync.display()\nDisplay the content in a rich console.\n\n\n\ntypes.ChatResponseAsync.get_string()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html#attributes",
    "href": "reference/types.ChatResponseAsync.html#attributes",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html#properties",
    "href": "reference/types.ChatResponseAsync.html#properties",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "consumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/types.ChatResponseAsync.html#methods",
    "href": "reference/types.ChatResponseAsync.html#methods",
    "title": "types.ChatResponseAsync",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndisplay\nDisplay the content in a rich console.\n\n\nget_string\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponseAsync.display()\nDisplay the content in a rich console.\n\n\n\ntypes.ChatResponseAsync.get_string()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponseAsync"
    ]
  },
  {
    "objectID": "reference/Provider.html",
    "href": "reference/Provider.html",
    "title": "Provider",
    "section": "",
    "text": "Provider\nProvider()\nA model provider interface for a Chat.\nThis abstract class defines the interface a model provider must implement in order to be used with a Chat instance. The provider is responsible for performing the actual chat completion, and for handling the streaming of the completion results.\nNote that this class is exposed for developers who wish to implement their own provider. In general, you should not need to interact with this class directly.",
    "crumbs": [
      "Reference",
      "Implement a model provider",
      "Provider"
    ]
  },
  {
    "objectID": "reference/types.SubmitInputArgsT.html",
    "href": "reference/types.SubmitInputArgsT.html",
    "title": "types.SubmitInputArgsT",
    "section": "",
    "text": "types.SubmitInputArgsT\ntypes.SubmitInputArgsT\nA TypedDict representing the arguments that can be passed to the .chat() method of a Chat instance.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.SubmitInputArgsT"
    ]
  },
  {
    "objectID": "reference/Tool.html",
    "href": "reference/Tool.html",
    "title": "Tool",
    "section": "",
    "text": "Tool(self, func, *, model=None)\nDefine a tool\nDefine a Python function for use by a chatbot. The function will always be invoked in the current Python process.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[…, Any] | Callable[…, Awaitable[Any]]\nThe function to be invoked when the tool is called.\nrequired\n\n\nmodel\nOptional[type[BaseModel]]\nA Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function’s type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\nNone",
    "crumbs": [
      "Reference",
      "Tool calling",
      "Tool"
    ]
  },
  {
    "objectID": "reference/Tool.html#parameters",
    "href": "reference/Tool.html#parameters",
    "title": "Tool",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[…, Any] | Callable[…, Awaitable[Any]]\nThe function to be invoked when the tool is called.\nrequired\n\n\nmodel\nOptional[type[BaseModel]]\nA Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function’s type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\nNone",
    "crumbs": [
      "Reference",
      "Tool calling",
      "Tool"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html",
    "href": "reference/types.ChatResponse.html",
    "title": "types.ChatResponse",
    "section": "",
    "text": "types.ChatResponse(self, generator)\nChat response object.\nAn object that, when displayed, will simulatenously consume (if not already consumed) and display the response in a streaming fashion.\nThis is useful for interactive use: if the object is displayed, it can be viewed as it is being generated. And, if the object is not displayed, it can act like an iterator that can be consumed by something else.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.\n\n\n\n\n\n\nconsumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).\n\n\n\n\n\n\nName\nDescription\n\n\n\n\ndisplay\nDisplay the content in a rich console.\n\n\nget_string\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponse.display()\nDisplay the content in a rich console.\nThis method gets called automatically when the object is displayed.\n\n\n\ntypes.ChatResponse.get_string()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html#attributes",
    "href": "reference/types.ChatResponse.html#attributes",
    "title": "types.ChatResponse",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ncontent\nstr\nThe content of the chat response.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html#properties",
    "href": "reference/types.ChatResponse.html#properties",
    "title": "types.ChatResponse",
    "section": "",
    "text": "consumed Whether the response has been consumed. If the response has been fully consumed, then it can no longer be iterated over, but the content can still be retrieved (via the content attribute).",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/types.ChatResponse.html#methods",
    "href": "reference/types.ChatResponse.html#methods",
    "title": "types.ChatResponse",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndisplay\nDisplay the content in a rich console.\n\n\nget_string\nGet the chat response content as a string.\n\n\n\n\n\ntypes.ChatResponse.display()\nDisplay the content in a rich console.\nThis method gets called automatically when the object is displayed.\n\n\n\ntypes.ChatResponse.get_string()\nGet the chat response content as a string.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ChatResponse"
    ]
  },
  {
    "objectID": "reference/token_usage.html",
    "href": "reference/token_usage.html",
    "title": "token_usage",
    "section": "",
    "text": "token_usage()\nReport on token usage in the current session\nCall this function to find out the cumulative number of tokens that you have sent and received in the current session.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[TokenUsage] | None\nA list of dictionaries with the following keys: “name”, “input”, and “output”. If no tokens have been logged, then None is returned.",
    "crumbs": [
      "Reference",
      "Query token usage",
      "token_usage"
    ]
  },
  {
    "objectID": "reference/token_usage.html#returns",
    "href": "reference/token_usage.html#returns",
    "title": "token_usage",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nlist[TokenUsage] | None\nA list of dictionaries with the following keys: “name”, “input”, and “output”. If no tokens have been logged, then None is returned.",
    "crumbs": [
      "Reference",
      "Query token usage",
      "token_usage"
    ]
  },
  {
    "objectID": "reference/types.Content.html",
    "href": "reference/types.Content.html",
    "title": "types.Content",
    "section": "",
    "text": "types.Content\ntypes.Content()\nBase class for all content types that can be appear in a Turn",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.Content"
    ]
  },
  {
    "objectID": "reference/image_url.html",
    "href": "reference/image_url.html",
    "title": "content_image_url",
    "section": "",
    "text": "content_image_url(url, detail='auto')\nEncode image content from a URL for chat input.\nThis function is used to prepare image URLs for input to the chatbot. It can handle both regular URLs and data URLs.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid."
  },
  {
    "objectID": "reference/image_url.html#parameters",
    "href": "reference/image_url.html#parameters",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'"
  },
  {
    "objectID": "reference/image_url.html#returns",
    "href": "reference/image_url.html#returns",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object."
  },
  {
    "objectID": "reference/image_url.html#examples",
    "href": "reference/image_url.html#examples",
    "title": "content_image_url",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)"
  },
  {
    "objectID": "reference/image_url.html#raises",
    "href": "reference/image_url.html#raises",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Start a chat with a particular large language model (llm) provider.\n\n\n\nChatAnthropic\nChat with an Anthropic Claude model.\n\n\nChatAzureOpenAI\nChat with a model hosted on Azure OpenAI.\n\n\nChatBedrockAnthropic\nChat with an AWS bedrock model.\n\n\nChatGithub\nChat with a model hosted on the GitHub model marketplace.\n\n\nChatGoogle\nChat with a Google Gemini model.\n\n\nChatGroq\nChat with a model hosted on Groq.\n\n\nChatOllama\nChat with a local Ollama model.\n\n\nChatOpenAI\nChat with an OpenAI model.\n\n\nChatPerplexity\nChat with a model hosted on perplexity.ai.\n\n\n\n\n\n\nMethods and attributes available on a chat instance\n\n\n\nChat\nA chat object that can be used to interact with a language model.\n\n\n\n\n\n\nSubmit image input to the chat\n\n\n\ncontent_image_file\nEncode image content from a file for chat input.\n\n\ncontent_image_plot\nEncode the current matplotlib plot as an image for chat input.\n\n\ncontent_image_url\nEncode image content from a URL for chat input.\n\n\n\n\n\n\nAdd context to python function before registering it as a tool.\n\n\n\nTool\nDefine a tool\n\n\n\n\n\n\nA provider-agnostic representation of content generated during an assistant/user turn.\n\n\n\nTurn\nA user or assistant turn\n\n\n\n\n\n\n\n\n\ntoken_usage\nReport on token usage in the current session\n\n\n\n\n\n\n\n\n\nProvider\nA model provider interface for a Chat.\n\n\n\n\n\n\n\n\n\ntypes.Content\nBase class for all content types that can be appear in a Turn\n\n\ntypes.ContentImage\n\n\n\ntypes.ContentImageInline\n\n\n\ntypes.ContentImageRemote\n\n\n\ntypes.ContentJson\n\n\n\ntypes.ContentText\n\n\n\ntypes.ContentToolRequest\n\n\n\ntypes.ContentToolResult\n\n\n\ntypes.ChatResponse\nChat response object.\n\n\ntypes.ChatResponseAsync\nChat response (async) object.\n\n\ntypes.ImageContentTypes\nAllowable content types for images.\n\n\ntypes.MISSING_TYPE\nA singleton representing a missing value.\n\n\ntypes.MISSING\n\n\n\ntypes.SubmitInputArgsT\nA TypedDict representing the arguments that can be passed to the .chat()\n\n\ntypes.TokenUsage\nToken usage for a given provider (name).",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#chat-model-providers",
    "href": "reference/index.html#chat-model-providers",
    "title": "Function reference",
    "section": "",
    "text": "Start a chat with a particular large language model (llm) provider.\n\n\n\nChatAnthropic\nChat with an Anthropic Claude model.\n\n\nChatAzureOpenAI\nChat with a model hosted on Azure OpenAI.\n\n\nChatBedrockAnthropic\nChat with an AWS bedrock model.\n\n\nChatGithub\nChat with a model hosted on the GitHub model marketplace.\n\n\nChatGoogle\nChat with a Google Gemini model.\n\n\nChatGroq\nChat with a model hosted on Groq.\n\n\nChatOllama\nChat with a local Ollama model.\n\n\nChatOpenAI\nChat with an OpenAI model.\n\n\nChatPerplexity\nChat with a model hosted on perplexity.ai.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#the-chat-object",
    "href": "reference/index.html#the-chat-object",
    "title": "Function reference",
    "section": "",
    "text": "Methods and attributes available on a chat instance\n\n\n\nChat\nA chat object that can be used to interact with a language model.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#image-input",
    "href": "reference/index.html#image-input",
    "title": "Function reference",
    "section": "",
    "text": "Submit image input to the chat\n\n\n\ncontent_image_file\nEncode image content from a file for chat input.\n\n\ncontent_image_plot\nEncode the current matplotlib plot as an image for chat input.\n\n\ncontent_image_url\nEncode image content from a URL for chat input.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#tool-calling",
    "href": "reference/index.html#tool-calling",
    "title": "Function reference",
    "section": "",
    "text": "Add context to python function before registering it as a tool.\n\n\n\nTool\nDefine a tool",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#turns",
    "href": "reference/index.html#turns",
    "title": "Function reference",
    "section": "",
    "text": "A provider-agnostic representation of content generated during an assistant/user turn.\n\n\n\nTurn\nA user or assistant turn",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#query-token-usage",
    "href": "reference/index.html#query-token-usage",
    "title": "Function reference",
    "section": "",
    "text": "token_usage\nReport on token usage in the current session",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#implement-a-model-provider",
    "href": "reference/index.html#implement-a-model-provider",
    "title": "Function reference",
    "section": "",
    "text": "Provider\nA model provider interface for a Chat.",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#user-facing-types",
    "href": "reference/index.html#user-facing-types",
    "title": "Function reference",
    "section": "",
    "text": "types.Content\nBase class for all content types that can be appear in a Turn\n\n\ntypes.ContentImage\n\n\n\ntypes.ContentImageInline\n\n\n\ntypes.ContentImageRemote\n\n\n\ntypes.ContentJson\n\n\n\ntypes.ContentText\n\n\n\ntypes.ContentToolRequest\n\n\n\ntypes.ContentToolResult\n\n\n\ntypes.ChatResponse\nChat response object.\n\n\ntypes.ChatResponseAsync\nChat response (async) object.\n\n\ntypes.ImageContentTypes\nAllowable content types for images.\n\n\ntypes.MISSING_TYPE\nA singleton representing a missing value.\n\n\ntypes.MISSING\n\n\n\ntypes.SubmitInputArgsT\nA TypedDict representing the arguments that can be passed to the .chat()\n\n\ntypes.TokenUsage\nToken usage for a given provider (name).",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/types.ContentToolResult.html",
    "href": "reference/types.ContentToolResult.html",
    "title": "types.ContentToolResult",
    "section": "",
    "text": "types.ContentToolResult\ntypes.ContentToolResult(self, id, value=None, error=None)",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolResult"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html",
    "href": "reference/ChatGroq.html",
    "title": "ChatGroq",
    "section": "",
    "text": "ChatGroq(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://api.groq.com/openai/v1',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on Groq.\nGroq provides a platform for highly efficient AI inference.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nSign up at https://groq.com to get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGroq requires the openai package (e.g., pip install openai).\n\n\n\n\n\nimport os\nfrom chatlas import ChatGroq\n\nchat = ChatGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GROQ_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Groq’s API.\n'https://api.groq.com/openai/v1'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nThis function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for groq.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatGroq(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGROQ_API_KEY=...\nfrom chatlas import ChatGroq\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGroq()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GROQ_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#prerequisites",
    "href": "reference/ChatGroq.html#prerequisites",
    "title": "ChatGroq",
    "section": "",
    "text": "API key\n\n\n\nSign up at https://groq.com to get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGroq requires the openai package (e.g., pip install openai).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#examples",
    "href": "reference/ChatGroq.html#examples",
    "title": "ChatGroq",
    "section": "",
    "text": "import os\nfrom chatlas import ChatGroq\n\nchat = ChatGroq(api_key=os.getenv(\"GROQ_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#parameters",
    "href": "reference/ChatGroq.html#parameters",
    "title": "ChatGroq",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GROQ_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Groq’s API.\n'https://api.groq.com/openai/v1'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#returns",
    "href": "reference/ChatGroq.html#returns",
    "title": "ChatGroq",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#note",
    "href": "reference/ChatGroq.html#note",
    "title": "ChatGroq",
    "section": "",
    "text": "This function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for groq.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/ChatGroq.html#note-1",
    "href": "reference/ChatGroq.html#note-1",
    "title": "ChatGroq",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatGroq(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGROQ_API_KEY=...\nfrom chatlas import ChatGroq\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGroq()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GROQ_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGroq"
    ]
  },
  {
    "objectID": "reference/types.ContentImageRemote.html",
    "href": "reference/types.ContentImageRemote.html",
    "title": "types.ContentImageRemote",
    "section": "",
    "text": "types.ContentImageRemote\ntypes.ContentImageRemote(self, url, detail='')",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImageRemote"
    ]
  },
  {
    "objectID": "reference/types.MISSING.html",
    "href": "reference/types.MISSING.html",
    "title": "types.MISSING",
    "section": "",
    "text": "types.MISSING\ntypes.MISSING",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.MISSING"
    ]
  },
  {
    "objectID": "reference/types.ContentText.html",
    "href": "reference/types.ContentText.html",
    "title": "types.ContentText",
    "section": "",
    "text": "types.ContentText\ntypes.ContentText(self, text)",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentText"
    ]
  },
  {
    "objectID": "reference/types.ContentToolRequest.html",
    "href": "reference/types.ContentToolRequest.html",
    "title": "types.ContentToolRequest",
    "section": "",
    "text": "types.ContentToolRequest\ntypes.ContentToolRequest(self, id, name, arguments)",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentToolRequest"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html",
    "href": "reference/content_image_plot.html",
    "title": "content_image_plot",
    "section": "",
    "text": "content_image_plot(width=768, height=768, dpi=72)\nEncode the current matplotlib plot as an image for chat input.\nThis function captures the current matplotlib plot, resizes it to the specified dimensions, and prepares it for chat input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#parameters",
    "href": "reference/content_image_plot.html#parameters",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#returns",
    "href": "reference/content_image_plot.html#returns",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#raises",
    "href": "reference/content_image_plot.html#raises",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/content_image_plot.html#examples",
    "href": "reference/content_image_plot.html#examples",
    "title": "content_image_plot",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_plot"
    ]
  },
  {
    "objectID": "reference/Chat.html",
    "href": "reference/Chat.html",
    "title": "Chat",
    "section": "",
    "text": "Chat(self, provider, turns=None)\nA chat object that can be used to interact with a language model.\nA Chat is an sequence of sequence of user and assistant Turns sent to a specific Provider. A Chat takes care of managing the state associated with the chat; i.e. it records the messages that you send to the server, and the messages that you receive back. If you register a tool (i.e. an function that the assistant can call on your behalf), it also takes care of the tool loop.\nYou should generally not create this object yourself, but instead call ChatOpenAI or friends instead.\n\n\n\n\n\nName\nDescription\n\n\n\n\nsystem_prompt\nGet the system prompt for the chat.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\napp\nEnter a chat browser to interact with the LLM.\n\n\nchat\nGenerate a response from the chat.\n\n\nchat_async\nGenerate a response from the chat asynchronously.\n\n\nconsole\nEnter a chat console to interact with the LLM.\n\n\nextract_data\nExtract structured data from the given input.\n\n\nextract_data_async\nExtract structured data from the given input asynchronously.\n\n\nlast_turn\nGet the last turn in the chat with a specific role.\n\n\nregister_tool\nRegister a tool (function) with the chat.\n\n\nset_turns\nSet the turns of the chat.\n\n\ntokens\nGet the tokens for each turn in the chat.\n\n\nturns\nGet all the turns (i.e., message contents) in the chat.\n\n\n\n\n\nChat.app(stream=True, launch_browser=True, port=0, kwargs=None)\nEnter a chat browser to interact with the LLM.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nlaunch_browser\nbool\nWhether to launch a browser window.\nTrue\n\n\nport\nint\nThe port to run the app on (the default is 0, which will choose a random port).\n0\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\nChat.chat(*args, stream=True, kwargs=None)\nGenerate a response from the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponse\nA response from the chat.\n\n\n\n\n\n\n\nChat.chat_async(*args, stream=True, kwargs=None)\nGenerate a response from the chat asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\nChat.console(stream=True, kwargs=None)\nEnter a chat console to interact with the LLM.\nPress Ctrl+C to quit.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nChat.extract_data(*args, data_model)\nExtract structured data from the given input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.extract_data_async(*args, data_model)\nExtract structured data from the given input asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.last_turn(role='assistant')\nGet the last turn in the chat with a specific role.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrole\nLiteral['assistant', 'user', 'system']\nThe role of the turn to return.\n'assistant'\n\n\n\n\n\n\n\nChat.register_tool(func, *, model=None)\nRegister a tool (function) with the chat.\nThe function will always be invoked in the current Python process.\n\n\nIf your tool has straightforward input parameters, you can just register the function directly (type hints and a docstring explaning both what the function does and what the parameters are for is strongly recommended):\nfrom chatlas import ChatOpenAI, Tool\n\n\ndef add(a: int, b: int) -&gt; int:\n    '''\n    Add two numbers together.\n\n####     Parameters {.doc-section .doc-section-----parameters}\n\n    a : int\n        The first number to add.\n    b : int\n        The second number to add.\n    '''\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add)\nchat.chat(\"What is 2 + 2?\")\nIf your tool has more complex input parameters, you can provide a Pydantic model that corresponds to the input parameters for the function, This way, you can have fields that hold other model(s) (for more complex input parameters), and also more directly document the input parameters:\nfrom chatlas import ChatOpenAI, Tool\nfrom pydantic import BaseModel, Field\n\n\nclass AddParams(BaseModel):\n    '''Add two numbers together.'''\n\n    a: int = Field(description=\"The first number to add.\")\n\n    b: int = Field(description=\"The second number to add.\")\n\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add, model=AddParams)\nchat.chat(\"What is 2 + 2?\")\n\n\n\n\n\nfunc The function to be invoked when the tool is called. model A Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function’s type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\n\n\nChat.set_turns(turns)\nSet the turns of the chat.\nThis method is primarily useful for clearing or setting the turns of the chat (i.e., limiting the context window).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nturns\nSequence[Turn]\nThe turns to set. Turns with the role “system” are not allowed.\nrequired\n\n\n\n\n\n\n\nChat.tokens()\nGet the tokens for each turn in the chat.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[tuple[int, int]]\nA list of tuples, where each tuple contains the start and end token indices for a turn.\n\n\n\n\n\n\n\nChat.turns(include_system_prompt=False)\nGet all the turns (i.e., message contents) in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude_system_prompt\nbool\nWhether to include the system prompt in the turns.\nFalse",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#attributes",
    "href": "reference/Chat.html#attributes",
    "title": "Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nsystem_prompt\nGet the system prompt for the chat.",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#methods",
    "href": "reference/Chat.html#methods",
    "title": "Chat",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\napp\nEnter a chat browser to interact with the LLM.\n\n\nchat\nGenerate a response from the chat.\n\n\nchat_async\nGenerate a response from the chat asynchronously.\n\n\nconsole\nEnter a chat console to interact with the LLM.\n\n\nextract_data\nExtract structured data from the given input.\n\n\nextract_data_async\nExtract structured data from the given input asynchronously.\n\n\nlast_turn\nGet the last turn in the chat with a specific role.\n\n\nregister_tool\nRegister a tool (function) with the chat.\n\n\nset_turns\nSet the turns of the chat.\n\n\ntokens\nGet the tokens for each turn in the chat.\n\n\nturns\nGet all the turns (i.e., message contents) in the chat.\n\n\n\n\n\nChat.app(stream=True, launch_browser=True, port=0, kwargs=None)\nEnter a chat browser to interact with the LLM.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nlaunch_browser\nbool\nWhether to launch a browser window.\nTrue\n\n\nport\nint\nThe port to run the app on (the default is 0, which will choose a random port).\n0\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\nChat.chat(*args, stream=True, kwargs=None)\nGenerate a response from the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChatResponse\nA response from the chat.\n\n\n\n\n\n\n\nChat.chat_async(*args, stream=True, kwargs=None)\nGenerate a response from the chat asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe user input(s) to generate a response from.\n()\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response.\nNone\n\n\n\n\n\n\n\nChat.console(stream=True, kwargs=None)\nEnter a chat console to interact with the LLM.\nPress Ctrl+C to quit.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstream\nbool\nWhether to stream the response (i.e., have the response appear in chunks).\nTrue\n\n\nkwargs\nOptional[SubmitInputArgsT]\nAdditional keyword arguments to pass to the method used for requesting the response\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nNone\n\n\n\n\n\n\n\n\nChat.extract_data(*args, data_model)\nExtract structured data from the given input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.extract_data_async(*args, data_model)\nExtract structured data from the given input asynchronously.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nContent | str\nThe input to extract data from.\n()\n\n\ndata_model\ntype[BaseModel]\nA Pydantic model describing the structure of the data to extract.\nrequired\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ndict[str, Any]\nThe extracted data.\n\n\n\n\n\n\n\nChat.last_turn(role='assistant')\nGet the last turn in the chat with a specific role.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrole\nLiteral['assistant', 'user', 'system']\nThe role of the turn to return.\n'assistant'\n\n\n\n\n\n\n\nChat.register_tool(func, *, model=None)\nRegister a tool (function) with the chat.\nThe function will always be invoked in the current Python process.\n\n\nIf your tool has straightforward input parameters, you can just register the function directly (type hints and a docstring explaning both what the function does and what the parameters are for is strongly recommended):\nfrom chatlas import ChatOpenAI, Tool\n\n\ndef add(a: int, b: int) -&gt; int:\n    '''\n    Add two numbers together.\n\n####     Parameters {.doc-section .doc-section-----parameters}\n\n    a : int\n        The first number to add.\n    b : int\n        The second number to add.\n    '''\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add)\nchat.chat(\"What is 2 + 2?\")\nIf your tool has more complex input parameters, you can provide a Pydantic model that corresponds to the input parameters for the function, This way, you can have fields that hold other model(s) (for more complex input parameters), and also more directly document the input parameters:\nfrom chatlas import ChatOpenAI, Tool\nfrom pydantic import BaseModel, Field\n\n\nclass AddParams(BaseModel):\n    '''Add two numbers together.'''\n\n    a: int = Field(description=\"The first number to add.\")\n\n    b: int = Field(description=\"The second number to add.\")\n\n\ndef add(a: int, b: int) -&gt; int:\n    return a + b\n\n\nchat = ChatOpenAI()\nchat.register_tool(add, model=AddParams)\nchat.chat(\"What is 2 + 2?\")",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/Chat.html#parameters-7",
    "href": "reference/Chat.html#parameters-7",
    "title": "Chat",
    "section": "",
    "text": "func The function to be invoked when the tool is called. model A Pydantic model that describes the input parameters for the function. If not provided, the model will be inferred from the function’s type hints. The primary reason why you might want to provide a model in Note that the name and docstring of the model takes precedence over the name and docstring of the function.\n\n\nChat.set_turns(turns)\nSet the turns of the chat.\nThis method is primarily useful for clearing or setting the turns of the chat (i.e., limiting the context window).\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nturns\nSequence[Turn]\nThe turns to set. Turns with the role “system” are not allowed.\nrequired\n\n\n\n\n\n\n\nChat.tokens()\nGet the tokens for each turn in the chat.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[tuple[int, int]]\nA list of tuples, where each tuple contains the start and end token indices for a turn.\n\n\n\n\n\n\n\nChat.turns(include_system_prompt=False)\nGet all the turns (i.e., message contents) in the chat.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude_system_prompt\nbool\nWhether to include the system prompt in the turns.\nFalse",
    "crumbs": [
      "Reference",
      "The chat object",
      "Chat"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html",
    "href": "reference/ChatOllama.html",
    "title": "ChatOllama",
    "section": "",
    "text": "ChatOllama(\n    model=None,\n    *,\n    system_prompt=None,\n    turns=None,\n    base_url='http://localhost:11434/v1',\n    seed=None,\n    kwargs=None,\n)\nChat with a local Ollama model.\nOllama makes it easy to run a wide-variety of open-source models locally, making it a great choice for privacy and security.\n\n\n\n\n\n\n\n\nOllama runtime\n\n\n\nChatOllama requires the ollama executable to be installed and running on your machine.\n\n\n\n\n\n\n\n\nPull model(s)\n\n\n\nOnce ollama is running locally, download a model from the command line (e.g. ollama pull llama3.2).\n\n\n\n\n\nfrom chatlas import ChatOllama\n\nchat = ChatOllama(model=\"llama3.2\")\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat. If None, a list of locally installed models will be printed.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses ollama’s API.\n'http://localhost:11434/v1'\n\n\nseed\nOptional[int]\nOptional integer seed that helps to make output more reproducible.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\nThis function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for ollama.\n\n\n\nChatOllama currently doesn’t work with streaming tools, and tool calling more generally doesn’t seem to work very well with currently available models.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#prerequisites",
    "href": "reference/ChatOllama.html#prerequisites",
    "title": "ChatOllama",
    "section": "",
    "text": "Ollama runtime\n\n\n\nChatOllama requires the ollama executable to be installed and running on your machine.\n\n\n\n\n\n\n\n\nPull model(s)\n\n\n\nOnce ollama is running locally, download a model from the command line (e.g. ollama pull llama3.2).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#examples",
    "href": "reference/ChatOllama.html#examples",
    "title": "ChatOllama",
    "section": "",
    "text": "from chatlas import ChatOllama\n\nchat = ChatOllama(model=\"llama3.2\")\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#parameters",
    "href": "reference/ChatOllama.html#parameters",
    "title": "ChatOllama",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat. If None, a list of locally installed models will be printed.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses ollama’s API.\n'http://localhost:11434/v1'\n\n\nseed\nOptional[int]\nOptional integer seed that helps to make output more reproducible.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#note",
    "href": "reference/ChatOllama.html#note",
    "title": "ChatOllama",
    "section": "",
    "text": "This function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for ollama.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "reference/ChatOllama.html#limitations",
    "href": "reference/ChatOllama.html#limitations",
    "title": "ChatOllama",
    "section": "",
    "text": "ChatOllama currently doesn’t work with streaming tools, and tool calling more generally doesn’t seem to work very well with currently available models.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOllama"
    ]
  },
  {
    "objectID": "web-apps.html",
    "href": "web-apps.html",
    "title": "chatlas",
    "section": "",
    "text": "In the intro, we learned how the .app() method launches a web app with a simple chat interface, for example:\nThis is a great way to quickly test your model, but you’ll likely want to embed similar functionality into a larger web app. Here’s how you can do that we different web frameworks."
  },
  {
    "objectID": "web-apps.html#shiny",
    "href": "web-apps.html#shiny",
    "title": "chatlas",
    "section": "Shiny",
    "text": "Shiny\nPass the result of .chat() directly to Shiny’s ui.Chat component to create a chat interface in your own Shiny app.\nfrom chatlas import ChatAnthropic\nfrom shiny import ui\n\nchat = ui.Chat(\n  id=\"chat\", \n  messages=[\"Hi! How can I help you today?\"],\n)\n\nchat_model = ChatAnthropic()\n\n@chat.on_user_submit\ndef _():\n    response = chat_model.chat(chat.user_input())\n    chat.append_message_stream(response)"
  },
  {
    "objectID": "web-apps.html#streamlit",
    "href": "web-apps.html#streamlit",
    "title": "chatlas",
    "section": "Streamlit",
    "text": "Streamlit\nComing soon"
  },
  {
    "objectID": "structured-data.html",
    "href": "structured-data.html",
    "title": "Structured data",
    "section": "",
    "text": "When using an LLM to extract data from text or images, you can ask the chatbot to nicely format it, in JSON or any other format that you like. This will generally work well most of the time, but there’s no gaurantee that you’ll actually get the exact format that you want. In particular, if you’re trying to get JSON, find that it’s typically surrounded in ```json, and you’ll occassionally get text that isn’t actually valid JSON. To avoid these challenges you can use a recent LLM feature: structured data (aka structured output). With structured data, you supply a type specification that exactly defines the object structure that you want and the LLM will guarantee that’s what you get back.\nimport json\nimport pandas as pd\nfrom chatlas import ChatOpenAI\nfrom pydantic import BaseModel, Field"
  },
  {
    "objectID": "structured-data.html#structured-data-basics",
    "href": "structured-data.html#structured-data-basics",
    "title": "Structured data",
    "section": "Structured data basics",
    "text": "Structured data basics\nTo extract structured data you call the .extract_data() method instead of the .chat() method. You’ll also need to define a type specification that describes the structure of the data that you want (more on that shortly). Here’s a simple example that extracts two specific values from a string:\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n\nchat = ChatOpenAI()\nchat.extract_data(\n  \"My name is Susan and I'm 13 years old\", \n  data_model=Person,\n)\n\n{'name': 'Susan', 'age': 13}\n\n\nThe same basic idea works with images too:\n\nfrom chatlas import content_image_url\n\nclass Image(BaseModel):\n    primary_shape: str\n    primary_colour: str\n\nchat.extract_data(\n  content_image_url(\"https://www.r-project.org/Rlogo.png\"),\n  data_model=Image,\n)\n\n{'primary_shape': 'oval', 'primary_colour': 'blue'}"
  },
  {
    "objectID": "structured-data.html#data-types-basics",
    "href": "structured-data.html#data-types-basics",
    "title": "Structured data",
    "section": "Data types basics",
    "text": "Data types basics\nTo define your desired type specification (also known as a schema), you use a pydantic model.\nIn addition to the model definition with field names and types, you may also want to provide the LLM with an additional context about what each field/model represents. In this case, include a Field(description=\"...\") for each field, and a docstring for each model. This is a good place to ask nicely for other attributes you’ll like the value to possess (e.g. minimum or maximum values, date formats, …). You aren’t guaranteed that these requests will be honoured, but the LLM will usually make a best effort to do so.\n\nclass Person(BaseModel):\n    \"\"\"A person\"\"\"\n\n    name: str = Field(description=\"Name\")\n\n    age: int = Field(description=\"Age, in years\")\n\n    hobbies: list[str] = Field(\n        description=\"List of hobbies. Should be exclusive and brief.\"\n    )\n\nNow we’ll dive into some examples before coming back to talk more data types details."
  },
  {
    "objectID": "structured-data.html#examples",
    "href": "structured-data.html#examples",
    "title": "Structured data",
    "section": "Examples",
    "text": "Examples\nThe following examples are closely inspired by the Claude documentation and hint at some of the ways you can use structured data extraction.\n\nExample 1: Article summarisation\n\nwith open(\"examples/third-party-testing.txt\") as f:\n    text = f.read()\n\n\nclass ArticleSummary(BaseModel):\n    \"\"\"Summary of the article.\"\"\"\n\n    author: str = Field(description=\"Name of the article author\")\n\n    topics: list[str] = Field(\n        description=\"Array of topics, e.g. ['tech', 'politics']. Should be as specific as possible, and can overlap.\"\n    )\n\n    summary: str = Field(description=\"Summary of the article. One or two paragraphs max\")\n\n    coherence: int = Field(\n        description=\"Coherence of the article's key points, 0-100 (inclusive)\"\n    )\n\n    persuasion: float = Field(\n        description=\"Article's persuasion score, 0.0-1.0 (inclusive)\"\n    )\n\n\nchat = ChatOpenAI()\ndata = chat.extract_data(text, data_model=ArticleSummary)\nprint(json.dumps(data, indent=2))\n\n{\n  \"author\": \"Anthropic\",\n  \"topics\": [\n    \"AI policy\",\n    \"third-party testing\",\n    \"AI safety\",\n    \"regulatory capture\",\n    \"national security\"\n  ],\n  \"summary\": \"The article advocates for effective third-party testing of frontier AI systems to mitigate risks and societal harm from powerful AI technologies. It discusses the necessity of developing a robust testing regime, driven by insights from industry, government, and academia, to address challenges such as election integrity and misuse of AI for harmful purposes. The authors argue that establishing such a framework is crucial for gaining public trust in AI, ensuring that testing is manageable for small organizations, and fostering international collaboration on standards.\",\n  \"coherence\": 90,\n  \"persuasion\": 0.85\n}\n\n\n\n\nExample 2: Named entity recognition\n\ntext = \"John works at Google in New York. He met with Sarah, the CEO of Acme Inc., last week in San Francisco.\"\n\n\nclass NamedEntity(BaseModel):\n    \"\"\"Named entity in the text.\"\"\"\n\n    name: str = Field(description=\"The extracted entity name\")\n\n    type_: str = Field(description=\"The entity type, e.g. 'person', 'location', 'organization'\")\n\n    context: str = Field(description=\"The context in which the entity appears in the text.\")\n\n\nclass NamedEntities(BaseModel):\n    \"\"\"Named entities in the text.\"\"\"\n\n    entities: list[NamedEntity] = Field(description=\"Array of named entities\")\n\n\nchat = ChatOpenAI()\ndata = chat.extract_data(text, data_model=NamedEntities)\npd.DataFrame(data[\"entities\"])\n\n\n\n\n\n\n\n\nname\ntype_\ncontext\n\n\n\n\n0\nJohn\nperson\nWorks at Google in New York.\n\n\n1\nGoogle\norganization\nWhere John works.\n\n\n2\nNew York\nlocation\nCity where John works.\n\n\n3\nSarah\nperson\nThe CEO of Acme Inc.\n\n\n4\nAcme Inc.\norganization\nCompany where Sarah is CEO.\n\n\n5\nSan Francisco\nlocation\nWhere John met with Sarah.\n\n\n\n\n\n\n\n\n\nExample 3: Sentiment analysis\n\ntext = \"The product was okay, but the customer service was terrible. I probably won't buy from them again.\"\n\nclass Sentiment(BaseModel):\n    \"\"\"Extract the sentiment scores of a given text. Sentiment scores should sum to 1.\"\"\"\n\n    positive_score: float = Field(\n        description=\"Positive sentiment score, ranging from 0.0 to 1.0\"\n    )\n\n    negative_score: float = Field(\n        description=\"Negative sentiment score, ranging from 0.0 to 1.0\"\n    )\n\n    neutral_score: float = Field(\n        description=\"Neutral sentiment score, ranging from 0.0 to 1.0\"\n    )\n\n\nchat = ChatOpenAI()\nchat.extract_data(text, data_model=Sentiment)\n\n{'positive_score': 0.2, 'negative_score': 0.6, 'neutral_score': 0.2}\n\n\nNote that we’ve asked nicely for the scores to sum 1, and they do in this example (at least when I ran the code), but it’s not guaranteed.\n\n\nExample 4: Text classification\n\nfrom typing import Literal\n\ntext = \"The new quantum computing breakthrough could revolutionize the tech industry.\"\n\n\nclass Classification(BaseModel):\n\n    name: Literal[\n        \"Politics\", \"Sports\", \"Technology\", \"Entertainment\", \"Business\", \"Other\"\n    ] = Field(description=\"The category name\")\n\n    score: float = Field(description=\"The classification score for the category, ranging from 0.0 to 1.0.\")\n\n\nclass Classifications(BaseModel):\n    \"\"\"Array of classification results. The scores should sum to 1.\"\"\"\n\n    classifications: list[Classification]\n\n\nchat = ChatOpenAI()\ndata = chat.extract_data(text, data_model=Classifications)\npd.DataFrame(data[\"classifications\"])\n\n\n\n\n\n\n\n\nname\nscore\n\n\n\n\n0\nTechnology\n0.9\n\n\n1\nBusiness\n0.1\n\n\n\n\n\n\n\n\n\nExample 5: Working with unknown keys\n\nfrom chatlas import ChatAnthropic\n\n\nclass Characteristics(BaseModel, extra=\"allow\"):\n    \"\"\"All characteristics\"\"\"\n\n    pass\n\n\nprompt = \"\"\"\n  Given a description of a character, your task is to extract all the characteristics of that character.\n\n  &lt;description&gt;\n  The man is tall, with a beard and a scar on his left cheek. He has a deep voice and wears a black leather jacket.\n  &lt;/description&gt;\n\"\"\"\n\nchat = ChatAnthropic()\ndata = chat.extract_data(prompt, data_model=Characteristics)\nprint(json.dumps(data, indent=2))\n\n{\n  \"physical_characteristics\": {\n    \"height\": \"tall\",\n    \"facial_features\": {\n      \"beard\": true,\n      \"scar\": {\n        \"location\": \"left cheek\"\n      }\n    }\n  },\n  \"voice\": {\n    \"quality\": \"deep\"\n  },\n  \"clothing\": {\n    \"outerwear\": {\n      \"type\": \"leather jacket\",\n      \"color\": \"black\"\n    }\n  }\n}\n\n\nThis example only works with Claude, not GPT or Gemini, because only Claude supports adding arbitrary additional properties.\n\n\nExample 6: Extracting data from an image\nThis example comes from Dan Nguyen and you can see other interesting applications at that link.\nThe goal is to extract structured data from this screenshot:\n\n\n\nA screenshot of schedule A: a table showing assets and “unearned” income\n\n\nEven without any descriptions, ChatGPT does pretty well:\n\nfrom chatlas import content_image_file\n\n\nclass Asset(BaseModel):\n    assert_name: str\n    owner: str\n    location: str\n    asset_value_low: int\n    asset_value_high: int\n    income_type: str\n    income_low: int\n    income_high: int\n    tx_gt_1000: bool\n\n\nclass DisclosureReport(BaseModel):\n    assets: list[Asset]\n\n\nchat = ChatOpenAI()\ndata = chat.extract_data(\n    content_image_file(\"images/congressional-assets.png\"), data_model=DisclosureReport\n)\npd.DataFrame(data[\"assets\"])\n\n\n\n\n\n\n\n\nassert_name\nowner\nlocation\nasset_value_low\nasset_value_high\nincome_type\nincome_low\nincome_high\ntx_gt_1000\n\n\n\n\n0\n11 Zinfandel Lane - Home & Vineyard\nJT\nSt. Helena/Napa, CA, US\n5000001\n45000000\nGrape Sales\n100001\n1000000\nTrue\n\n\n1\n25 Point Lobos - Commercial Property\nSP\nSan Francisco, CA, US\n6000001\n45000000\nRent\n100001\n1000000\nTrue"
  },
  {
    "objectID": "structured-data.html#advanced-data-types",
    "href": "structured-data.html#advanced-data-types",
    "title": "Structured data",
    "section": "Advanced data types",
    "text": "Advanced data types\nNow that you’ve seen a few examples, it’s time to get into more specifics about data type declarations.\n\nRequired vs optional\nBy default, model fields are in a sense “required”, unless None is allowed in their type definition. Including None is a good idea if there’s any possibility of the input not containing the required fields as LLMs may hallucinate data in order to fulfill your spec.\nFor example, here the LLM hallucinates a date even though there isn’t one in the text:\n\nclass ArticleSpec(BaseModel):\n    \"\"\"Information about an article written in markdown\"\"\"\n\n    title: str = Field(description=\"Article title\")\n    author: str = Field(description=\"Name of the author\")\n    date: str = Field(description=\"Date written in YYYY-MM-DD format.\")\n\n\nprompt = \"\"\"\n  Extract data from the following text:\n\n  &lt;text&gt;\n  # Structured Data\n  By Hadley Wickham\n\n  When using an LLM to extract data from text or images, you can ask the chatbot to nicely format it, in JSON or any other format that you like.\n  &lt;/text&gt;\n\"\"\"\n\nchat = ChatOpenAI()\ndata = chat.extract_data(prompt, data_model=ArticleSpec)\nprint(json.dumps(data, indent=2))\n\n{\n  \"title\": \"Structured Data\",\n  \"author\": \"Hadley Wickham\",\n  \"date\": \"2023-10-08\"\n}\n\n\nNote that I’ve used more of an explict prompt here. For this example, I found that this generated better results, and it’s a useful place to put additional instructions.\nIf let the LLM know that the fields are all optional, it’ll instead return None for the missing fields:\n\nclass ArticleSpec(BaseModel):\n    \"\"\"Information about an article written in markdown\"\"\"\n\n    title: str = Field(description=\"Article title\")\n    author: str = Field(description=\"Name of the author\")\n    date: str | None = Field(description=\"Date written in YYYY-MM-DD format.\")\n\n\ndata = chat.extract_data(prompt, data_model=ArticleSpec)\nprint(json.dumps(data, indent=2))\n\n{\n  \"title\": \"Structured Data\",\n  \"author\": \"Hadley Wickham\",\n  \"date\": null\n}\n\n\n\n\nData frames\nIf you want to define a data frame like data_model, you might be tempted to create a model like this, where each field is a list of scalar values:\nclass Persons(BaseModel):\n    name: list[str]\n    age: list[int]\nThis however, is not quite right because there’s no way to specify that each field should have the same length. Instead you need to turn the data structure “inside out”, and instead create an array of objects:\nclass Person(BaseModel):\n    name: str\n    age: int\n\nclass Persons(BaseModel):\n    persons: list[Person]\nIf you’re familiar with the terms between row-oriented and column-oriented data frames, this is the same idea."
  },
  {
    "objectID": "structured-data.html#token-usage",
    "href": "structured-data.html#token-usage",
    "title": "Structured data",
    "section": "Token usage",
    "text": "Token usage\nBelow is a summary of the tokens used to create the output in this example.\n\nfrom chatlas import token_usage\ntoken_usage()\n\n[{'name': 'OpenAI', 'input': 22571, 'output': 501},\n {'name': 'Anthropic', 'input': 463, 'output': 148}]"
  },
  {
    "objectID": "prompt-engineering.html",
    "href": "prompt-engineering.html",
    "title": "Prompt design",
    "section": "",
    "text": "import chatlas"
  },
  {
    "objectID": "prompt-engineering.html#general-advice",
    "href": "prompt-engineering.html#general-advice",
    "title": "Prompt design",
    "section": "General advice",
    "text": "General advice\nIf you’ve never written a prompt before, the easiest way to think about it is precisely explaining what you want a technically skilled but naive human what you want. The key is clearly stating what you want, resolving any potential ambiguities, and providing a few examples. And indeed writing the prompt can often help you too, because it forces you to be explicit about what you want. You should expect to iterate multiple times before you get to a good prompt, but you’ll learn a lot along the way. It’s good practice to keep your prompt in Git (so you can record exactly how it’s changing) and build up a small set of challenge examples that you can use to verify that the prompt does what you expect. (Eventually, you might want to formally evaluate different prompts for the problem you’re tackling, but that’s currently outside the scope of elmer and this vignette).\nBecause prompts can be quite long, we suggest writing them in markdown. That way you can use headers to divide up the prompt, and other tools like itemised lists to enumerate multiple options.\n\nShiny assistant prompt\nSidebot\nHadley recipe prompt\nhttps://github.com/simonpcouch/pal/tree/main/inst/prompts\n\nIt’s also a good idea to read the advice for the specific model that you’re using, as there may be specific tweaks that work well with just that model. Here are some pointers to the prompt engineering guides for a few popular models:\n\nClaude\nOpenAI\nGemini"
  },
  {
    "objectID": "prompt-engineering.html#directing-behavioroutput",
    "href": "prompt-engineering.html#directing-behavioroutput",
    "title": "Prompt design",
    "section": "Directing behavior/output",
    "text": "Directing behavior/output\n\nBasic flavour\n\nchat = chatlas.ChatOpenAI(system_prompt = \"Be concise. Don't use punctuation.\")\n_ = chat.chat(\"What's the capital of New Zealand?\")\n\n\nchat = chatlas.ChatOpenAI(\n    system_prompt=\"\"\"\n  Be loquacious.\n  Enjoy using florid turns of phrase.\n  Drop in extra historical information even when not asked for it.\n\"\"\"\n)\n_ = chat.chat(\"What's the capital of New Zealand?\")\n\n\nchat = chatlas.ChatOpenAI(\n    system_prompt=\"\"\"\n  You are an expert Python programmer.\n  Just give me the code; no explanation in text.\n  But include a few comments in line explaining particularly complex operations.\n\"\"\"\n)\n_ = chat.chat(\"\"\"\n  How can I compute the mean and median of variables a, b, c, ...\n  grouped by age and sex\n\"\"\")\n\n\n\nBe clear and specific\nIf you’re not getting the results that you’re looking for, you might need to provide more context.\n\nchat = chatlas.ChatOpenAI(\n    system_prompt=\"\"\"\n  You are an expert Python programmer.\n  You prefer to use polars and siuba when possible.\n  Just give me the code; no explanation.\n\"\"\"\n)\n_ = chat.chat(\"\"\"\n  How can I compute the mean and median of variables a, b, c, ...\n  grouped by age and sex\n\"\"\")\n\n\n\nTeach it about new features\n\nchat = chatlas.ChatOpenAI(\n    system_prompt=\"\"\"\n  dplyr 1.1.0 introduced per-operation grouping with the `.by` argument.\n  You job is to convert code that uses group_by() to code that uses `.by`.\n\n  Some examples:\n\n  # Before\n  transactions |&gt;\n    group_by(company, year) |&gt;\n    mutate(total = sum(revenue))\n\n  # After\n  transactions |&gt;\n    mutate(\n      total = sum(revenue),\n      .by = c(company, year)\n    )\n\"\"\"\n)\nchat.chat(\"\"\"\n  How can I compute the mean and median of variables a, b, c, ...\n  grouped by age and sex. Just show me the code.\n\"\"\")\n\nChatResponse object. Call `.display()` to show it in a richconsole or `.get_string()` to get the content.\n\n\nExamples are a powerful technique that we’ll discuss in more detail below.\n\n\nChain of thought\nYou can often get better responses if you ask the LLM to explain it’s work, or begin by asking it to come up with a plan of attack. Because these answers are included in the chat history, and resent to the model on each turn, the assistant can use it’s own explanations to improve future answers.\n\n\nUncertainty\n\n“Be careful to only provide answers that you are sure about. If you are uncertain about an answer, say so.”\n\n\n\nOther tools\nIf you have a claude account, you can use its https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/prompt-generator. This prompt generator has been specifically tailored for Claude, but I suspect it will help many other LLMs, or at least give you some ideas as to what else you might want to include in your prompt."
  },
  {
    "objectID": "prompt-engineering.html#using-examples-to-guide-behavior",
    "href": "prompt-engineering.html#using-examples-to-guide-behavior",
    "title": "Prompt design",
    "section": "Using examples to guide behavior",
    "text": "Using examples to guide behavior\nYou’ll also hear this called single-shot or multi-shot prompting.\nNote, however, there are some\n\nGoal: Extract ingredient list from recipe and return in a structured format.\nExample user input:\nIn a large bowl, cream together 1 cup of softened unsalted butter and ½ cup of white sugar until smooth. Beat in 1 egg and 1 teaspoon of vanilla extract. Gradually stir in 2 cups of all-purpose flour until the dough forms. Finally, fold in 1 cup of semisweet chocolate chips. Drop spoonfuls of dough onto an ungreased baking sheet and bake at 350°F (175°C) for 10-12 minutes, or until the edges are lightly browned. Let the cookies cool on the baking sheet for a few minutes before transferring to a wire rack to cool completely. Enjoy!\n\n\nSystem prompt 1: just instructions\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\nAssistant response:\n{\n  \"ingredients\": [\n    \"1 cup of softened unsalted butter\",\n    \"½ cup of white sugar\",\n    \"1 egg\",\n    \"1 teaspoon of vanilla extract\",\n    \"2 cups of all-purpose flour\",\n    \"1 cup of semisweet chocolate chips\"\n  ]\n}\n\n\nSystem prompt 2: detailed description of output\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format. It should be an array of objects, where each object has keys `ingredient`, `quantity`, and `unit`. Put each object on one line of output.\nAssistant response:\n[\n    {\"ingredient\": \"unsalted butter\", \"quantity\": 1, \"unit\": \"cup\"},\n    {\"ingredient\": \"white sugar\", \"quantity\": 1/2, \"unit\": \"cup\"},\n    {\"ingredient\": \"egg\", \"quantity\": 1, \"unit\": \"large\"},\n    {\"ingredient\": \"vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\"},\n    {\"ingredient\": \"all-purpose flour\", \"quantity\": 2, \"unit\": \"cups\"},\n    {\"ingredient\": \"semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\"}\n]\n\n\nSystem prompt 3: one-shot example\nThe user input contains a recipe. Extract a list of ingredients and return it in JSON format.\n\nExample Output:\n\n```json\n[\n    { \"ingredient\": \"Flour\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"Vegetable oil\", \"quantity\": 0.5, \"unit\": \"tsp\" },\n    { \"ingredient\": \"Onion\", \"quantity\": 1, \"unit\": null },\n]\n```\nAssistant response:\n[\n    { \"ingredient\": \"Unsalted butter\", \"quantity\": 1, \"unit\": \"cup\" },\n    { \"ingredient\": \"White sugar\", \"quantity\": 0.5, \"unit\": \"cup\" },\n    { \"ingredient\": \"Egg\", \"quantity\": 1, \"unit\": null },\n    { \"ingredient\": \"Vanilla extract\", \"quantity\": 1, \"unit\": \"teaspoon\" },\n    { \"ingredient\": \"All-purpose flour\", \"quantity\": 2, \"unit\": \"cups\" },\n    { \"ingredient\": \"Semisweet chocolate chips\", \"quantity\": 1, \"unit\": \"cup\" }\n]"
  },
  {
    "objectID": "prompt-engineering.html#adding-contextknowledge-to-prompt",
    "href": "prompt-engineering.html#adding-contextknowledge-to-prompt",
    "title": "Prompt design",
    "section": "Adding context/knowledge to prompt",
    "text": "Adding context/knowledge to prompt\n\nAdd documentation files to prompt\nAdd positive examples (negative examples don’t work well)\nDocs must fit in context window\nExamples\n\nSidebot\nFastHTML LLM prompt\nElmer assistant uses README files in prompt"
  },
  {
    "objectID": "tool-calling.html",
    "href": "tool-calling.html",
    "title": "Introduction",
    "section": "",
    "text": "One of the most interesting aspects of modern chat models is their ability to make use of external tools that are defined by the caller.\nWhen making a chat request to the chat model, the caller advertises one or more tools (defined by their function name, description, and a list of expected arguments), and the chat model can choose to respond with one or more “tool calls”. These tool calls are requests from the chat model to the caller to execute the function with the given arguments; the caller is expected to execute the functions and “return” the results by submitting another chat request with the conversation so far, plus the results. The chat model can then use those results in formulating its response, or, it may decide to make additional tool calls.\nNote that the chat model does not directly execute any external tools! It only makes requests for the caller to execute them. It’s easy to think that tool calling might work like this:\n\n\n\nDiagram showing showing the wrong mental model of tool calls: a user initiates a request that flows to the assistant, which then runs the code, and returns the result back to the user.”\n\n\nBut in fact it works like this:\n\n\n\nDiagram showing the correct mental model for tool calls: a user sends a request that needs a tool call, the assistant request that the user’s runs that tool, returns the result to the assistant, which uses it to generate the final answer.\n\n\nThe value that the chat model brings is not in helping with execution, but with knowing when it makes sense to call a tool, what values to pass as arguments, and how to use the results in formulating its response.\n\nfrom chatlas import ChatOpenAI\n\n\nMotivating example\nLet’s take a look at an example where we really need an external tool. Chat models generally do not know the current time, which makes questions like these impossible.\n\nchat = ChatOpenAI(model=\"gpt-4o\")\n_ = chat.chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\")\n\nUnfortunately, the LLM doesn’t hallucinates the current date. Let’s give the chat model the ability to determine the current time and try again.\n\n\nDefining a tool function\nThe first thing we’ll do is define a Python function that returns the current time. This will be our tool.\n\ndef get_current_time(tz: str = \"UTC\") -&gt; str:\n    \"\"\"\n    Gets the current time in the given time zone.\n\n    Parameters\n    ----------\n    tz\n        The time zone to get the current time in. Defaults to \"UTC\".\n\n    Returns\n    -------\n    str\n        The current time in the given time zone.\n    \"\"\"\n    from datetime import datetime\n    from zoneinfo import ZoneInfo\n\n    return datetime.now(ZoneInfo(tz)).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n\nNote that we’ve gone through the trouble of adding the following to our function:\n\nType hints for arguments and the return value\nA docstring that explains what the function does and what arguments it expects\n\nProviding these hints and context is very important, as it helps the chat model understand how to use your tool correctly!\nLet’s test it:\n\nget_current_time()\n\n'2024-11-15 16:03:27 UTC'\n\n\n\n\nUsing the tool\nIn order for the LLM to make use of our tool, we need to register it with the chat object. This is done by calling the register_tool method on the chat object.\n\nchat.register_tool(get_current_time)\n\nNow let’s retry our original question:\n\n_ = chat.chat(\"How long ago exactly was the moment Neil Armstrong touched down on the moon?\")\n\nThat’s correct! Without any further guidance, the chat model decided to call our tool function and successfully used its result in formulating its response.\n(Full disclosure: I originally tried this example with the default model of gpt-4o-mini and it got the tool calling right but the date math wrong, hence the explicit model=\"gpt-4o\".)\nThis tool example was extremely simple, but you can imagine doing much more interesting things from tool functions: calling APIs, reading from or writing to a database, kicking off a complex simulation, or even calling a complementary GenAI model (like an image generator). Or if you are using chatlas in a Shiny app, you could use tools to set reactive values, setting off a chain of reactive updates.\n\n\nTool limitations\nRemember that tool arguments come from the chat model, and tool results are returned to the chat model. That means that only simple, JSON-compatible data types can be used as inputs and outputs. It’s highly recommended that you stick to basic types for each function parameter (e.g. str, float/int, bool, None, list, tuple, dict). And you can forget about using functions, classes, external pointers, and other complex (i.e., non-serializable) Python objects as arguments or return values. Returning data frames seems to work OK (as long as you return the JSON representation – .to_json()), although be careful not to return too much data, as it all counts as tokens (i.e., they count against your context window limit and also cost you money)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "chatlas",
    "section": "",
    "text": "Easily chat with various LLM models from Ollama, Anthropic, OpenAI, and more. chatlas is intentionally minimal – making it easy to get started, while also supporting important features like streaming, tool calling, images, async, and more.\n(Looking for something similar to chatlas, but in R? Check out elmer!)\n\n\nchatlas isn’t yet on pypi, but you can install from Github:\npip install git+https://github.com/posit-dev/chatlas\nAfter installing, you’ll want to pick a model provider, and get credentials set up (if necessary). Here, we demonstrate usage with OpenAI, but the concepts here apply to other implementations as well.\n\n\n\nchatlas supports a variety of model providers:\n\nAnthropic (Claude): ChatAnthropic().\nGitHub model marketplace: ChatGithub().\nGoogle (Gemini): ChatGoogle().\nGroq: ChatGroq().\nOllama local models: ChatOllama().\nOpenAI: ChatOpenAI().\nperplexity.ai: ChatPerplexity().\n\nIt also supports the following enterprise cloud providers:\n\nAWS Bedrock: ChatBedrockAnthropic().\nAzure OpenAI: ChatAzureOpenAI().\n\n\n\n\nIf you’re using chatlas inside your organisation, you’ll typically need to use whatever you’re allowed to. If you’re using chatlas for your own personal exploration, we recommend starting with:\nChatOpenAI(), which currently defaults to model=\"gpt-4o-mini\". You might want to try \"gpt-4o\" for more demanding task and if you want to force complex reasoning, \"o1-mini\".\nChatAnthropic(), which defaults to Claude 3.5 Sonnet. This currently appears to be the best model for code generation.\nIf you want to put a lot of data in the prompt, try ChatGoogle() which defaults to Gemini 1.5 Flash and supports 1 million tokens, compared to 200k for Claude 3.5 Sonnet and 128k for GPT 4o mini.\n\n\n\nYou can chat via chatlas in several different ways, depending on whether you are working interactively or programmatically. They all start with creating a new chat object:\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(\n  model = \"gpt-4o-mini\",\n  system_prompt = \"You are a friendly but terse assistant.\",\n)\nChat objects are stateful: they retain the context of the conversation, so each new query can build on the previous ones. This is true regardless of which of the various ways of chatting you use.\n\n\nFrom a chat instance, you can start an interacitve, multi-turn, conversation in the console (via .console()) or in a browser (via .app()).\nchat.console()\nEntering chat console. Press Ctrl+C to quit.\n\n?&gt; Who created Python?\n\nPython was created by Guido van Rossum. He began development in the late 1980s and released the first     \nversion in 1991. \n\n?&gt; Where did he develop it?\n\nGuido van Rossum developed Python while working at Centrum Wiskunde & Informatica (CWI) in the            \nNetherlands.     \nThe chat console is useful for quickly exploring the capabilities of the model, especially when you’ve customized the chat object with tool integrations (covered later).\nThe chat app is similar to the chat console, but it runs in your browser. It’s useful if you need more interactive capabilities like easy copy-paste.\nchat.app()\n\n\n\nAgain, keep in mind that the chat object retains state, so when you enter the chat console, any previous interactions with that chat object are still part of the conversation, and any interactions you have in the chat console will persist even after you exit back to the Python prompt.\n\n\n\nFor a more programmatic approach, you can use the .chat() method to ask a question and get a response. If you’re in a REPL (e.g., Jupyter, IPython, etc), the result of .chat() is automatically displayed using a rich console.\nchat.chat(\"What preceding languages most influenced Python?\")\nPython was primarily influenced by ABC, with additional inspiration from C,\nModula-3, and various other languages.\nIf you’re not in a REPL (e.g., a non-interactive Python script), you can explicitly .display() the response:\nresponse = chat.chat(\"What is the Python programming language?\")\nresponse.display()\nThe response is also an iterable, so you can loop over it to get the response in streaming chunks:\nresult = \"\"\nfor chunk in response:\n    result += chunk\nOr, if you just want the full response as a string, use the built-in str() function:\nstr(response)\n\n\n\nAsk questions about image(s) with content_image_file() and/or content_image_url():\nfrom chatlas import content_image_url\n\nchat.chat(\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n    \"Can you explain this logo?\"\n)\nThe Python logo features two intertwined snakes in yellow and blue,\nrepresenting the Python programming language. The design symbolizes...\nThe content_image_url() function takes a URL to an image file and sends that URL directly to the API. The content_image_file() function takes a path to a local image file and encodes it as a base64 string to send to the API. Note that by default, content_image_file() automatically resizes the image to fit within 512x512 pixels; set the resize parameter to “high” if higher resolution is needed.\n\n\n\nRemember that regardless of how we interact with the model, the chat instance retains the conversation history, which you can access at any time:\nchat.turns()\nEach turn represents a either a user’s input or a model’s response. It holds all the avaliable information about content and metadata of the turn. This can be useful for debugging, logging, or for building more complex conversational interfaces.\nFor cost and efficiency reasons, you may want to alter the conversation history. Currently, the main way to do this is to .set_turns():\n# Remove all but the last two turns\nchat.set_turns(chat.turns()[-2:])\n\n\n\nIf you’re new to world LLMs, you might want to read the Get Started guide, which covers some basic concepts and terminology.\nOnce you’re comfortable with the basics, you can explore more advanced topics:\n\nCustomize the system prompt\nExtract structured data\nTool (function) calling\nBuild a web chat app\n\nThe API reference is also a useful overview of all the tooling available in chatlas, including starting examples and detailed descriptions."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "chatlas",
    "section": "",
    "text": "chatlas isn’t yet on pypi, but you can install from Github:\npip install git+https://github.com/posit-dev/chatlas\nAfter installing, you’ll want to pick a model provider, and get credentials set up (if necessary). Here, we demonstrate usage with OpenAI, but the concepts here apply to other implementations as well."
  },
  {
    "objectID": "index.html#model-providers",
    "href": "index.html#model-providers",
    "title": "chatlas",
    "section": "",
    "text": "chatlas supports a variety of model providers:\n\nAnthropic (Claude): ChatAnthropic().\nGitHub model marketplace: ChatGithub().\nGoogle (Gemini): ChatGoogle().\nGroq: ChatGroq().\nOllama local models: ChatOllama().\nOpenAI: ChatOpenAI().\nperplexity.ai: ChatPerplexity().\n\nIt also supports the following enterprise cloud providers:\n\nAWS Bedrock: ChatBedrockAnthropic().\nAzure OpenAI: ChatAzureOpenAI()."
  },
  {
    "objectID": "index.html#model-choice",
    "href": "index.html#model-choice",
    "title": "chatlas",
    "section": "",
    "text": "If you’re using chatlas inside your organisation, you’ll typically need to use whatever you’re allowed to. If you’re using chatlas for your own personal exploration, we recommend starting with:\nChatOpenAI(), which currently defaults to model=\"gpt-4o-mini\". You might want to try \"gpt-4o\" for more demanding task and if you want to force complex reasoning, \"o1-mini\".\nChatAnthropic(), which defaults to Claude 3.5 Sonnet. This currently appears to be the best model for code generation.\nIf you want to put a lot of data in the prompt, try ChatGoogle() which defaults to Gemini 1.5 Flash and supports 1 million tokens, compared to 200k for Claude 3.5 Sonnet and 128k for GPT 4o mini."
  },
  {
    "objectID": "index.html#using-chatlas",
    "href": "index.html#using-chatlas",
    "title": "chatlas",
    "section": "",
    "text": "You can chat via chatlas in several different ways, depending on whether you are working interactively or programmatically. They all start with creating a new chat object:\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(\n  model = \"gpt-4o-mini\",\n  system_prompt = \"You are a friendly but terse assistant.\",\n)\nChat objects are stateful: they retain the context of the conversation, so each new query can build on the previous ones. This is true regardless of which of the various ways of chatting you use.\n\n\nFrom a chat instance, you can start an interacitve, multi-turn, conversation in the console (via .console()) or in a browser (via .app()).\nchat.console()\nEntering chat console. Press Ctrl+C to quit.\n\n?&gt; Who created Python?\n\nPython was created by Guido van Rossum. He began development in the late 1980s and released the first     \nversion in 1991. \n\n?&gt; Where did he develop it?\n\nGuido van Rossum developed Python while working at Centrum Wiskunde & Informatica (CWI) in the            \nNetherlands.     \nThe chat console is useful for quickly exploring the capabilities of the model, especially when you’ve customized the chat object with tool integrations (covered later).\nThe chat app is similar to the chat console, but it runs in your browser. It’s useful if you need more interactive capabilities like easy copy-paste.\nchat.app()\n\n\n\nAgain, keep in mind that the chat object retains state, so when you enter the chat console, any previous interactions with that chat object are still part of the conversation, and any interactions you have in the chat console will persist even after you exit back to the Python prompt.\n\n\n\nFor a more programmatic approach, you can use the .chat() method to ask a question and get a response. If you’re in a REPL (e.g., Jupyter, IPython, etc), the result of .chat() is automatically displayed using a rich console.\nchat.chat(\"What preceding languages most influenced Python?\")\nPython was primarily influenced by ABC, with additional inspiration from C,\nModula-3, and various other languages.\nIf you’re not in a REPL (e.g., a non-interactive Python script), you can explicitly .display() the response:\nresponse = chat.chat(\"What is the Python programming language?\")\nresponse.display()\nThe response is also an iterable, so you can loop over it to get the response in streaming chunks:\nresult = \"\"\nfor chunk in response:\n    result += chunk\nOr, if you just want the full response as a string, use the built-in str() function:\nstr(response)\n\n\n\nAsk questions about image(s) with content_image_file() and/or content_image_url():\nfrom chatlas import content_image_url\n\nchat.chat(\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n    \"Can you explain this logo?\"\n)\nThe Python logo features two intertwined snakes in yellow and blue,\nrepresenting the Python programming language. The design symbolizes...\nThe content_image_url() function takes a URL to an image file and sends that URL directly to the API. The content_image_file() function takes a path to a local image file and encodes it as a base64 string to send to the API. Note that by default, content_image_file() automatically resizes the image to fit within 512x512 pixels; set the resize parameter to “high” if higher resolution is needed.\n\n\n\nRemember that regardless of how we interact with the model, the chat instance retains the conversation history, which you can access at any time:\nchat.turns()\nEach turn represents a either a user’s input or a model’s response. It holds all the avaliable information about content and metadata of the turn. This can be useful for debugging, logging, or for building more complex conversational interfaces.\nFor cost and efficiency reasons, you may want to alter the conversation history. Currently, the main way to do this is to .set_turns():\n# Remove all but the last two turns\nchat.set_turns(chat.turns()[-2:])\n\n\n\nIf you’re new to world LLMs, you might want to read the Get Started guide, which covers some basic concepts and terminology.\nOnce you’re comfortable with the basics, you can explore more advanced topics:\n\nCustomize the system prompt\nExtract structured data\nTool (function) calling\nBuild a web chat app\n\nThe API reference is also a useful overview of all the tooling available in chatlas, including starting examples and detailed descriptions."
  },
  {
    "objectID": "get-started.html",
    "href": "get-started.html",
    "title": "First, some useful LLM concepts",
    "section": "",
    "text": "The goal of this vignette is give you the key vocabulary you need in order to use LLMs effectively. In this package, we treat LLMs as black boxes, ignoring the details of the underlying models. This is because the details of the models aren’t particuarly useful at the level at which you’ll mostly want to work (in the same way that you need an extremely deep understaning of physics in order to inform your understand of chemistry), and it’s best to start with a highly empirical approach.\n(https://www.youtube.com/watch?v=sYliwvml9Es)\n\nToken: the fundamental component of the underlying model, important for understanding how much your query will cost, and how much data you can send in the prompt.\nPrompt: the text you send to the model.\nConversation: the sequence of conversational turns that alternate between the user and the assistant.\nProvider: the service that provides predictions\nModel: while some models are\n\nWe’ll then finish off with some examples of the types of thing that you can do with LLMs and chatlas to help get your creative juices flowing.\n\n\n\n\nA tokens are very important for the implementation of LLMs, because when combined with an embedding, they convert text to numbers, which are then used to train the model. Tokens are also important for you, because they are used to compute query cost and they how much information you can put in a prompt.\nIf you want to learn more about tokens, I’d recommend watching the first 20-30 minutes of Let’s build the GPT Tokenizer by Andrej Karpathy. You certainly don’t need to learn how to build your own tokenizer, but the intro will give you a bunch of useful background knowledge that will help improve your undersstanding of how LLM’s work.\nModels are priced according to input and output tokens. Models are priced per million tokens and vary a lot based from model-to-model. Mid-tir models (e.g. gpt-4o or claude 3 haiku) might be around $0.25 per million input and $1 per million output tokens; state of the art models (like gpt-4o or claude 3 sonnet) might be more like $2.50 per million input tokens, and $10 per million output tokens.\nCertainly, at the time of writing, even $10 of API credit will give you a lot of room for experimenting when using mid-tier models. It’s hard to predict how these costs will change in the future. In the short-term, it’s likely that costs will decrease as folks figure out more efficient implementations, but costs are currently highly subsidized by venture capital model, so you can expect costs to rise when that starts to run out.\nTo help calibrate, on average, an English word requires around 1.5 tokens. A page might be 375-400 tokens. A complete book might be 75,000 to 150,000 tokens. Other languages will often require more tokens LLMs are trained on data from the internet, which is primarily in English.\nYou’ll also hear about tokens in the context of “context length”, or how long your conversations can be.\nWith chatlas, you can see how many tokens a conversations has used when you print it, or you can see total usage for a session with token_usage().\n\n\n\nA conversation with an LLM takes place through a series of HTTP requests and responses: you send your question to the LLM in a HTTP request, and it sends its reply back in a HTTP response. In chatlas, we refer to these as conversational turns: a conversation is a sequence of turns between the user (you) and the assistant (the LLM).\nDepsite the fact that conversations are inherently stateful (i.e. your response to a question depends on the completely history of the conversation), LLM APIs are stateless. That means every time you send an question to an LLM, you have to actually send the entire conversation history. This is important to understand because:\n\nIt affects pricing. You are charged per token, so each question in a conversation is going to include all the previous questions and answers, meaning that the cost is going to grow quadratically with the number of questions. In other words, to save money, keep your conversational threads short.\nYou have full control over the conversational history. This means for example that you can start a conversation with one provider, and then send it to another provider.\nEvery response is affected by all previous questions and responses. You may want to start a new chat session if you don’t want this history to affect subsequent answers. And in general, it’s typically better to iterate on your system prompt (or first user prompt) rather than iterating through a long conversation with the model.\n\n\n\n\n\nUser prompt: a question or statement for the model to respond to\nSystem prompt: behind-the-scenes instructions and information for the model\nCore system prompt:\n\nIf you’re developing an app, then you’ll provide the system prompt and your users will provide their own prompts. If you’re developing a solution, you’ll provide both the system prompt and the user prompt.\nModels will also have their own systems prompts that are applied underneath your prompts. You can get a sense for what these look like from Anthropic, who publishes their system prompts.\nWriting good prompts is called prompt engineering and is key to effective use of LLMs. We’ll discuss that in more details in Prompt Engineering.\n\n\n\nA provider is a company that provides a model for use via API. Some providers are synonynous with a model: for example OpenAI and chatGPT, anthropic and Claude, and Google and Gemini.\nBut other providers host many different models. These models are typically open source models (like LLaMa, mistral). Bigger providers will often partner with one of the proprietary models: for example, Azure OpenAI offers a range of open source models plus OpenAI’s chatGPT, AWS Bedrock offers a range of open source models plus anthropic’s Claude models.\n\n\n\n\nUser interfaces:\n\nA chatbot with shinychat. From this simple framework you can add extra features, e.g. https://shiny.posit.co/blog/posts/shiny-assistant/.\nModify code/text using {rstudioapi}.\nA function that takes unstructured input and returns structured output. Learn more in Structured Data\n\nInteraction with the LLM:\n\nAutomate adding data to the prompt.\n\nFor example, you could automatically look up the documentation for a Python function, and include it in the prompt.\nDescribe a data frame. https://github.com/cpsievert/aidea\n\nCreate a long hand written prompt that teaches the LLM about something it wouldn’t otherwise know about. For example, you might write a guide to updating code to use a new version of a package.\n\nhttps://github.com/jcheng5/elmer-assistant\nhttps://simonpcouch.github.io/pal/\n\nConnect an LLM with more data by using tool calling. Learn more in Tool Calling\nDefine create a prompt with example of the structured data output that you want."
  },
  {
    "objectID": "get-started.html#vocabulary",
    "href": "get-started.html#vocabulary",
    "title": "First, some useful LLM concepts",
    "section": "",
    "text": "A tokens are very important for the implementation of LLMs, because when combined with an embedding, they convert text to numbers, which are then used to train the model. Tokens are also important for you, because they are used to compute query cost and they how much information you can put in a prompt.\nIf you want to learn more about tokens, I’d recommend watching the first 20-30 minutes of Let’s build the GPT Tokenizer by Andrej Karpathy. You certainly don’t need to learn how to build your own tokenizer, but the intro will give you a bunch of useful background knowledge that will help improve your undersstanding of how LLM’s work.\nModels are priced according to input and output tokens. Models are priced per million tokens and vary a lot based from model-to-model. Mid-tir models (e.g. gpt-4o or claude 3 haiku) might be around $0.25 per million input and $1 per million output tokens; state of the art models (like gpt-4o or claude 3 sonnet) might be more like $2.50 per million input tokens, and $10 per million output tokens.\nCertainly, at the time of writing, even $10 of API credit will give you a lot of room for experimenting when using mid-tier models. It’s hard to predict how these costs will change in the future. In the short-term, it’s likely that costs will decrease as folks figure out more efficient implementations, but costs are currently highly subsidized by venture capital model, so you can expect costs to rise when that starts to run out.\nTo help calibrate, on average, an English word requires around 1.5 tokens. A page might be 375-400 tokens. A complete book might be 75,000 to 150,000 tokens. Other languages will often require more tokens LLMs are trained on data from the internet, which is primarily in English.\nYou’ll also hear about tokens in the context of “context length”, or how long your conversations can be.\nWith chatlas, you can see how many tokens a conversations has used when you print it, or you can see total usage for a session with token_usage().\n\n\n\nA conversation with an LLM takes place through a series of HTTP requests and responses: you send your question to the LLM in a HTTP request, and it sends its reply back in a HTTP response. In chatlas, we refer to these as conversational turns: a conversation is a sequence of turns between the user (you) and the assistant (the LLM).\nDepsite the fact that conversations are inherently stateful (i.e. your response to a question depends on the completely history of the conversation), LLM APIs are stateless. That means every time you send an question to an LLM, you have to actually send the entire conversation history. This is important to understand because:\n\nIt affects pricing. You are charged per token, so each question in a conversation is going to include all the previous questions and answers, meaning that the cost is going to grow quadratically with the number of questions. In other words, to save money, keep your conversational threads short.\nYou have full control over the conversational history. This means for example that you can start a conversation with one provider, and then send it to another provider.\nEvery response is affected by all previous questions and responses. You may want to start a new chat session if you don’t want this history to affect subsequent answers. And in general, it’s typically better to iterate on your system prompt (or first user prompt) rather than iterating through a long conversation with the model.\n\n\n\n\n\nUser prompt: a question or statement for the model to respond to\nSystem prompt: behind-the-scenes instructions and information for the model\nCore system prompt:\n\nIf you’re developing an app, then you’ll provide the system prompt and your users will provide their own prompts. If you’re developing a solution, you’ll provide both the system prompt and the user prompt.\nModels will also have their own systems prompts that are applied underneath your prompts. You can get a sense for what these look like from Anthropic, who publishes their system prompts.\nWriting good prompts is called prompt engineering and is key to effective use of LLMs. We’ll discuss that in more details in Prompt Engineering.\n\n\n\nA provider is a company that provides a model for use via API. Some providers are synonynous with a model: for example OpenAI and chatGPT, anthropic and Claude, and Google and Gemini.\nBut other providers host many different models. These models are typically open source models (like LLaMa, mistral). Bigger providers will often partner with one of the proprietary models: for example, Azure OpenAI offers a range of open source models plus OpenAI’s chatGPT, AWS Bedrock offers a range of open source models plus anthropic’s Claude models."
  },
  {
    "objectID": "get-started.html#sample-use-cases",
    "href": "get-started.html#sample-use-cases",
    "title": "First, some useful LLM concepts",
    "section": "",
    "text": "User interfaces:\n\nA chatbot with shinychat. From this simple framework you can add extra features, e.g. https://shiny.posit.co/blog/posts/shiny-assistant/.\nModify code/text using {rstudioapi}.\nA function that takes unstructured input and returns structured output. Learn more in Structured Data\n\nInteraction with the LLM:\n\nAutomate adding data to the prompt.\n\nFor example, you could automatically look up the documentation for a Python function, and include it in the prompt.\nDescribe a data frame. https://github.com/cpsievert/aidea\n\nCreate a long hand written prompt that teaches the LLM about something it wouldn’t otherwise know about. For example, you might write a guide to updating code to use a new version of a package.\n\nhttps://github.com/jcheng5/elmer-assistant\nhttps://simonpcouch.github.io/pal/\n\nConnect an LLM with more data by using tool calling. Learn more in Tool Calling\nDefine create a prompt with example of the structured data output that you want."
  },
  {
    "objectID": "reference/ChatGithub.html",
    "href": "reference/ChatGithub.html",
    "title": "ChatGithub",
    "section": "",
    "text": "ChatGithub(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://models.inference.ai.azure.com/',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on the GitHub model marketplace.\nGitHub (via Azure) hosts a wide variety of open source models, some of which are fined tuned for specific tasks.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nSign up at https://github.com/marketplace/models to get an API key. You may need to apply for and be accepted into a beta access program.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGithub requires the openai package (e.g., pip install openai).\n\n\n\n\n\nimport os\nfrom chatlas import ChatGithub\n\nchat = ChatGithub(api_key=os.getenv(\"GITHUB_PAT\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GITHUB_PAT environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Github’s API.\n'https://models.inference.ai.azure.com/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nThis function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for the GitHub model marketplace.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatGithub(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGITHUB_PAT=...\nfrom chatlas import ChatGithub\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGithub()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GITHUB_PAT=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#prerequisites",
    "href": "reference/ChatGithub.html#prerequisites",
    "title": "ChatGithub",
    "section": "",
    "text": "API key\n\n\n\nSign up at https://github.com/marketplace/models to get an API key. You may need to apply for and be accepted into a beta access program.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGithub requires the openai package (e.g., pip install openai).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#examples",
    "href": "reference/ChatGithub.html#examples",
    "title": "ChatGithub",
    "section": "",
    "text": "import os\nfrom chatlas import ChatGithub\n\nchat = ChatGithub(api_key=os.getenv(\"GITHUB_PAT\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#parameters",
    "href": "reference/ChatGithub.html#parameters",
    "title": "ChatGithub",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GITHUB_PAT environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Github’s API.\n'https://models.inference.ai.azure.com/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#returns",
    "href": "reference/ChatGithub.html#returns",
    "title": "ChatGithub",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#note",
    "href": "reference/ChatGithub.html#note",
    "title": "ChatGithub",
    "section": "",
    "text": "This function is a lightweight wrapper around ChatOpenAI with the defaults tweaked for the GitHub model marketplace.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatGithub.html#note-1",
    "href": "reference/ChatGithub.html#note-1",
    "title": "ChatGithub",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatGithub(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGITHUB_PAT=...\nfrom chatlas import ChatGithub\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGithub()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GITHUB_PAT=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGithub"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html",
    "href": "reference/ChatAzureOpenAI.html",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "ChatAzureOpenAI(\n    endpoint,\n    deployment_id,\n    api_version,\n    api_key=None,\n    system_prompt=None,\n    turns=None,\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on Azure OpenAI.\nThe Azure OpenAI server hosts a number of open source models as well as proprietary models from OpenAI.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatAzureOpenAI requires the openai package (e.g., pip install openai).\n\n\n\n\n\nimport os\nfrom chatlas import ChatAzureOpenAI\n\nchat = ChatAzureOpenAI(\n    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    deployment_id=\"REPLACE_WITH_YOUR_DEPLOYMENT_ID\",\n    api_version=\"YYYY-MM-DD\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n)\n\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nendpoint\nstr\nAzure OpenAI endpoint url with protocol and hostname, i.e. https://{your-resource-name}.openai.azure.com. Defaults to using the value of the AZURE_OPENAI_ENDPOINT envinronment variable.\nrequired\n\n\ndeployment_id\nstr\nDeployment id for the model you want to use.\nrequired\n\n\napi_version\nstr\nThe API version to use.\nrequired\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the AZURE_OPENAI_API_KEY environment variable.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatAzureClientArgs']\nAdditional arguments to pass to the openai.AzureOpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#prerequisites",
    "href": "reference/ChatAzureOpenAI.html#prerequisites",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "Python requirements\n\n\n\nChatAzureOpenAI requires the openai package (e.g., pip install openai).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#examples",
    "href": "reference/ChatAzureOpenAI.html#examples",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "import os\nfrom chatlas import ChatAzureOpenAI\n\nchat = ChatAzureOpenAI(\n    endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n    deployment_id=\"REPLACE_WITH_YOUR_DEPLOYMENT_ID\",\n    api_version=\"YYYY-MM-DD\",\n    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n)\n\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#parameters",
    "href": "reference/ChatAzureOpenAI.html#parameters",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nendpoint\nstr\nAzure OpenAI endpoint url with protocol and hostname, i.e. https://{your-resource-name}.openai.azure.com. Defaults to using the value of the AZURE_OPENAI_ENDPOINT envinronment variable.\nrequired\n\n\ndeployment_id\nstr\nDeployment id for the model you want to use.\nrequired\n\n\napi_version\nstr\nThe API version to use.\nrequired\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the AZURE_OPENAI_API_KEY environment variable.\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatAzureClientArgs']\nAdditional arguments to pass to the openai.AzureOpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatAzureOpenAI.html#returns",
    "href": "reference/ChatAzureOpenAI.html#returns",
    "title": "ChatAzureOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAzureOpenAI"
    ]
  },
  {
    "objectID": "reference/Turn.html",
    "href": "reference/Turn.html",
    "title": "Turn",
    "section": "",
    "text": "Turn(self, role, contents, json=None, tokens=(0, 0))\nA user or assistant turn\nEvery conversation with a chatbot consists of pairs of user and assistant turns, corresponding to an HTTP request and response. These turns are represented by the Turn object, which contains a list of Contents representing the individual messages within the turn. These might be text, images, tool requests (assistant only), or tool responses (user only).\nNote that a call to .chat() and related functions may result in multiple user-assistant turn cycles. For example, if you have registered tools, chatlas will automatically handle the tool calling loop, which may result in any number of additional cycles.\n\n\nfrom chatlas import Turn, ChatOpenAI, ChatAnthropic\n\nchat = ChatOpenAI()\nstr(chat.chat(\"What is the capital of France?\"))\nturns = chat.turns()\nassert len(turns) == 2\nassert isinstance(turns[0], Turn)\nassert turns[0].role == \"user\"\nassert turns[1].role == \"assistant\"\n\n# Load context into a new chat instance\nchat2 = ChatAnthropic(turns=turns)\nturns2 = chat2.turns()\nassert turns == turns2\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nrole\nstr\nEither “user”, “assistant”, or “system”.\nrequired\n\n\ncontents\nstr | Sequence[Content | str]\nA list of Content objects.\nrequired\n\n\njson\nOptional[dict[str, Any]]\nThe serialized JSON corresponding to the underlying data of the turns. Currently only provided for assistant. This is useful if there’s information returned by the provider that chatlas doesn’t otherwise expose.\nNone\n\n\ntokens\ntuple[int, int]\nA numeric vector of length 2 representing the number of input and output tokens (respectively) used in this turn. Currently only recorded for assistant turns.\n(0, 0)",
    "crumbs": [
      "Reference",
      "Turns",
      "Turn"
    ]
  },
  {
    "objectID": "reference/Turn.html#examples",
    "href": "reference/Turn.html#examples",
    "title": "Turn",
    "section": "",
    "text": "from chatlas import Turn, ChatOpenAI, ChatAnthropic\n\nchat = ChatOpenAI()\nstr(chat.chat(\"What is the capital of France?\"))\nturns = chat.turns()\nassert len(turns) == 2\nassert isinstance(turns[0], Turn)\nassert turns[0].role == \"user\"\nassert turns[1].role == \"assistant\"\n\n# Load context into a new chat instance\nchat2 = ChatAnthropic(turns=turns)\nturns2 = chat2.turns()\nassert turns == turns2",
    "crumbs": [
      "Reference",
      "Turns",
      "Turn"
    ]
  },
  {
    "objectID": "reference/Turn.html#parameters",
    "href": "reference/Turn.html#parameters",
    "title": "Turn",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nrole\nstr\nEither “user”, “assistant”, or “system”.\nrequired\n\n\ncontents\nstr | Sequence[Content | str]\nA list of Content objects.\nrequired\n\n\njson\nOptional[dict[str, Any]]\nThe serialized JSON corresponding to the underlying data of the turns. Currently only provided for assistant. This is useful if there’s information returned by the provider that chatlas doesn’t otherwise expose.\nNone\n\n\ntokens\ntuple[int, int]\nA numeric vector of length 2 representing the number of input and output tokens (respectively) used in this turn. Currently only recorded for assistant turns.\n(0, 0)",
    "crumbs": [
      "Reference",
      "Turns",
      "Turn"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html",
    "href": "reference/ChatAnthropic.html",
    "title": "ChatAnthropic",
    "section": "",
    "text": "ChatAnthropic(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    max_tokens=4096,\n    kwargs=None,\n)\nChat with an Anthropic Claude model.\nAnthropic provides a number of chat based models under the Claude moniker.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nNote that a Claude Prop membership does not give you the ability to call models via the API. You will need to go to the developer console to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatAnthropic requires the anthropic package (e.g., pip install anthropic).\n\n\n\n\n\nimport os\nfrom chatlas import ChatAnthropic\n\nchat = ChatAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ModelParam]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the ANTHROPIC_API_KEY environment variable.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the anthropic.Anthropic() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.\n\n\n\n\n\n\nPasting an API key into a chat constructor (e.g., ChatAnthropic(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nANTHROPIC_API_KEY=...\nfrom chatlas import ChatAnthropic\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatAnthropic()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport ANTHROPIC_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#prerequisites",
    "href": "reference/ChatAnthropic.html#prerequisites",
    "title": "ChatAnthropic",
    "section": "",
    "text": "API key\n\n\n\nNote that a Claude Prop membership does not give you the ability to call models via the API. You will need to go to the developer console to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatAnthropic requires the anthropic package (e.g., pip install anthropic).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#examples",
    "href": "reference/ChatAnthropic.html#examples",
    "title": "ChatAnthropic",
    "section": "",
    "text": "import os\nfrom chatlas import ChatAnthropic\n\nchat = ChatAnthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#parameters",
    "href": "reference/ChatAnthropic.html#parameters",
    "title": "ChatAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ModelParam]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the ANTHROPIC_API_KEY environment variable.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the anthropic.Anthropic() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#returns",
    "href": "reference/ChatAnthropic.html#returns",
    "title": "ChatAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatAnthropic.html#note",
    "href": "reference/ChatAnthropic.html#note",
    "title": "ChatAnthropic",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatAnthropic(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nANTHROPIC_API_KEY=...\nfrom chatlas import ChatAnthropic\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatAnthropic()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport ANTHROPIC_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatAnthropic"
    ]
  },
  {
    "objectID": "reference/content_image_file.html",
    "href": "reference/content_image_file.html",
    "title": "content_image_file",
    "section": "",
    "text": "content_image_file(path, content_type='auto', resize='low')\nEncode image content from a file for chat input.\nThis function is used to prepare image files for input to the chatbot. It can handle various image formats and provides options for resizing.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[str, Literal['none', 'low', 'high']]\nResizing option for the image. Can be: - \"none\": No resizing - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\n'low'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#parameters",
    "href": "reference/content_image_file.html#parameters",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[str, Literal['none', 'low', 'high']]\nResizing option for the image. Can be: - \"none\": No resizing - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\n'low'",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#returns",
    "href": "reference/content_image_file.html#returns",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#examples",
    "href": "reference/content_image_file.html#examples",
    "title": "content_image_file",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/content_image_file.html#raises",
    "href": "reference/content_image_file.html#raises",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_file"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html",
    "href": "reference/ChatOpenAI.html",
    "title": "ChatOpenAI",
    "section": "",
    "text": "ChatOpenAI(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://api.openai.com/v1',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with an OpenAI model.\nOpenAI provides a number of chat based models under the ChatGPT moniker.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nNote that a ChatGPT Plus membership does not give you the ability to call models via the API. You will need to go to the developer platform to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatOpenAI requires the openai package (e.g., pip install openai).\n\n\n\n\n\nimport os\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ChatModel | str]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the OPENAI_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses OpenAI.\n'https://api.openai.com/v1'\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nPasting an API key into a chat constructor (e.g., ChatOpenAI(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nOPENAI_API_KEY=...\nfrom chatlas import ChatOpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatOpenAI()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport OPENAI_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#prerequisites",
    "href": "reference/ChatOpenAI.html#prerequisites",
    "title": "ChatOpenAI",
    "section": "",
    "text": "API key\n\n\n\nNote that a ChatGPT Plus membership does not give you the ability to call models via the API. You will need to go to the developer platform to sign up (and pay for) a developer account that will give you an API key that you can use with this package.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatOpenAI requires the openai package (e.g., pip install openai).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#examples",
    "href": "reference/ChatOpenAI.html#examples",
    "title": "ChatOpenAI",
    "section": "",
    "text": "import os\nfrom chatlas import ChatOpenAI\n\nchat = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#parameters",
    "href": "reference/ChatOpenAI.html#parameters",
    "title": "ChatOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\n'Optional[ChatModel | str]'\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the OPENAI_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses OpenAI.\n'https://api.openai.com/v1'\n\n\nseed\nint | None | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#returns",
    "href": "reference/ChatOpenAI.html#returns",
    "title": "ChatOpenAI",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatOpenAI.html#note",
    "href": "reference/ChatOpenAI.html#note",
    "title": "ChatOpenAI",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatOpenAI(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nOPENAI_API_KEY=...\nfrom chatlas import ChatOpenAI\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatOpenAI()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport OPENAI_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatOpenAI"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html",
    "href": "reference/ChatBedrockAnthropic.html",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "ChatBedrockAnthropic(\n    model=None,\n    max_tokens=4096,\n    aws_secret_key=None,\n    aws_access_key=None,\n    aws_region=None,\n    aws_profile=None,\n    aws_session_token=None,\n    base_url=None,\n    system_prompt=None,\n    turns=None,\n    kwargs=None,\n)\nChat with an AWS bedrock model.\nAWS Bedrock provides a number of chat based models, including those Anthropic’s Claude.\n\n\n\n\n\n\n\n\nAWS credentials\n\n\n\nConsider using the approach outlined in this guide to manage your AWS credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatBedrockAnthropic, requires the anthropic package with the bedrock extras (e.g., pip install anthropic[bedrock]).\n\n\n\n\n\nfrom chatlas import ChatBedrockAnthropic\n\nchat = ChatBedrockAnthropic(\n    aws_profile=\"...\",\n    aws_region=\"us-east\",\n    aws_secret_key=\"...\",\n    aws_access_key=\"...\",\n    aws_session_token=\"...\",\n)\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\naws_secret_key\nOptional[str]\nThe AWS secret key to use for authentication.\nNone\n\n\naws_access_key\nOptional[str]\nThe AWS access key to use for authentication.\nNone\n\n\naws_region\nOptional[str]\nThe AWS region to use. Defaults to the AWS_REGION environment variable. If that is not set, defaults to 'us-east-1'.\nNone\n\n\naws_profile\nOptional[str]\nThe AWS profile to use.\nNone\n\n\naws_session_token\nOptional[str]\nThe AWS session token to use.\nNone\n\n\nbase_url\nOptional[str]\nThe base URL to use. Defaults to the ANTHROPIC_BEDROCK_BASE_URL environment variable. If that is not set, defaults to f\"https://bedrock-runtime.{aws_region}.amazonaws.com\".\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nkwargs\nOptional['ChatBedrockClientArgs']\nAdditional arguments to pass to the anthropic.AnthropicBedrock() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#prerequisites",
    "href": "reference/ChatBedrockAnthropic.html#prerequisites",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "AWS credentials\n\n\n\nConsider using the approach outlined in this guide to manage your AWS credentials: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatBedrockAnthropic, requires the anthropic package with the bedrock extras (e.g., pip install anthropic[bedrock]).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#examples",
    "href": "reference/ChatBedrockAnthropic.html#examples",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "from chatlas import ChatBedrockAnthropic\n\nchat = ChatBedrockAnthropic(\n    aws_profile=\"...\",\n    aws_region=\"us-east\",\n    aws_secret_key=\"...\",\n    aws_access_key=\"...\",\n    aws_session_token=\"...\",\n)\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#parameters",
    "href": "reference/ChatBedrockAnthropic.html#parameters",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nmodel\nOptional[str]\nThe model to use for the chat.\nNone\n\n\nmax_tokens\nint\nMaximum number of tokens to generate before stopping.\n4096\n\n\naws_secret_key\nOptional[str]\nThe AWS secret key to use for authentication.\nNone\n\n\naws_access_key\nOptional[str]\nThe AWS access key to use for authentication.\nNone\n\n\naws_region\nOptional[str]\nThe AWS region to use. Defaults to the AWS_REGION environment variable. If that is not set, defaults to 'us-east-1'.\nNone\n\n\naws_profile\nOptional[str]\nThe AWS profile to use.\nNone\n\n\naws_session_token\nOptional[str]\nThe AWS session token to use.\nNone\n\n\nbase_url\nOptional[str]\nThe base URL to use. Defaults to the ANTHROPIC_BEDROCK_BASE_URL environment variable. If that is not set, defaults to f\"https://bedrock-runtime.{aws_region}.amazonaws.com\".\nNone\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nkwargs\nOptional['ChatBedrockClientArgs']\nAdditional arguments to pass to the anthropic.AnthropicBedrock() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/ChatBedrockAnthropic.html#returns",
    "href": "reference/ChatBedrockAnthropic.html#returns",
    "title": "ChatBedrockAnthropic",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatBedrockAnthropic"
    ]
  },
  {
    "objectID": "reference/image_file.html",
    "href": "reference/image_file.html",
    "title": "content_image_file",
    "section": "",
    "text": "content_image_file(path, content_type='auto', resize='low')\nEncode image content from a file for chat input.\nThis function is used to prepare image files for input to the chatbot. It can handle various image formats and provides options for resizing.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[str, Literal['none', 'low', 'high']]\nResizing option for the image. Can be: - \"none\": No resizing - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\n'low'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid."
  },
  {
    "objectID": "reference/image_file.html#parameters",
    "href": "reference/image_file.html#parameters",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\npath\nstr\nThe path to the image file to include in the chat input.\nrequired\n\n\ncontent_type\nLiteral['auto', ImageContentTypes]\nThe content type of the image (e.g., \"image/png\"). If \"auto\", the content type is inferred from the file extension.\n'auto'\n\n\nresize\nUnion[str, Literal['none', 'low', 'high']]\nResizing option for the image. Can be: - \"none\": No resizing - \"low\": Resize to fit within 512x512 - \"high\": Resize to fit within 2000x768 or 768x2000 - Custom string (e.g., \"200x200\", \"300x200&gt;!\", etc.)\n'low'"
  },
  {
    "objectID": "reference/image_file.html#returns",
    "href": "reference/image_file.html#returns",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object."
  },
  {
    "objectID": "reference/image_file.html#examples",
    "href": "reference/image_file.html#examples",
    "title": "content_image_file",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_file\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_file(\"path/to/image.png\"),\n)"
  },
  {
    "objectID": "reference/image_file.html#raises",
    "href": "reference/image_file.html#raises",
    "title": "content_image_file",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nFileNotFoundError\nIf the specified file does not exist.\n\n\n\nValueError\nIf the file extension is unsupported or the resize option is invalid."
  },
  {
    "objectID": "reference/types.MISSING_TYPE.html",
    "href": "reference/types.MISSING_TYPE.html",
    "title": "types.MISSING_TYPE",
    "section": "",
    "text": "types.MISSING_TYPE\ntypes.MISSING_TYPE()\nA singleton representing a missing value.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.MISSING_TYPE"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html",
    "href": "reference/ChatPerplexity.html",
    "title": "ChatPerplexity",
    "section": "",
    "text": "ChatPerplexity(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    base_url='https://api.perplexity.ai/',\n    seed=MISSING,\n    kwargs=None,\n)\nChat with a model hosted on perplexity.ai.\nPerplexity AI is a platform for running LLMs that are capable of searching the web in real-time to help them answer questions with information that may not have been available when the model was trained.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nSign up at https://www.perplexity.ai to get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatPerplexity requires the openai package (e.g., pip install openai).\n\n\n\n\n\nimport os\nfrom chatlas import ChatPerplexity\n\nchat = ChatPerplexity(api_key=os.getenv(\"PERPLEXITY_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the PERPLEXITY_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Perplexity’s API.\n'https://api.perplexity.ai/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.\n\n\n\n\n\n\nThis function is a lightweight wrapper around chatlas.ChatOpenAI with the defaults tweaked for perplexity.ai.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatPerplexity(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nPERPLEXITY_API_KEY=...\nfrom chatlas import ChatPerplexity\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatPerplexity()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport PERPLEXITY_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#prerequisites",
    "href": "reference/ChatPerplexity.html#prerequisites",
    "title": "ChatPerplexity",
    "section": "",
    "text": "API key\n\n\n\nSign up at https://www.perplexity.ai to get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatPerplexity requires the openai package (e.g., pip install openai).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#examples",
    "href": "reference/ChatPerplexity.html#examples",
    "title": "ChatPerplexity",
    "section": "",
    "text": "import os\nfrom chatlas import ChatPerplexity\n\nchat = ChatPerplexity(api_key=os.getenv(\"PERPLEXITY_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#parameters",
    "href": "reference/ChatPerplexity.html#parameters",
    "title": "ChatPerplexity",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the PERPLEXITY_API_KEY environment variable.\nNone\n\n\nbase_url\nstr\nThe base URL to the endpoint; the default uses Perplexity’s API.\n'https://api.perplexity.ai/'\n\n\nseed\nOptional[int] | MISSING_TYPE\nOptional integer seed that ChatGPT uses to try and make output more reproducible.\nMISSING\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the openai.OpenAI() client constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#returns",
    "href": "reference/ChatPerplexity.html#returns",
    "title": "ChatPerplexity",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA chat object that retains the state of the conversation.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#note",
    "href": "reference/ChatPerplexity.html#note",
    "title": "ChatPerplexity",
    "section": "",
    "text": "This function is a lightweight wrapper around chatlas.ChatOpenAI with the defaults tweaked for perplexity.ai.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/ChatPerplexity.html#note-1",
    "href": "reference/ChatPerplexity.html#note-1",
    "title": "ChatPerplexity",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatPerplexity(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nPERPLEXITY_API_KEY=...\nfrom chatlas import ChatPerplexity\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatPerplexity()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport PERPLEXITY_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatPerplexity"
    ]
  },
  {
    "objectID": "reference/types.ImageContentTypes.html",
    "href": "reference/types.ImageContentTypes.html",
    "title": "types.ImageContentTypes",
    "section": "",
    "text": "types.ImageContentTypes\ntypes.ImageContentTypes\nAllowable content types for images.",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ImageContentTypes"
    ]
  },
  {
    "objectID": "reference/types.ContentImage.html",
    "href": "reference/types.ContentImage.html",
    "title": "types.ContentImage",
    "section": "",
    "text": "types.ContentImage\ntypes.ContentImage()",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImage"
    ]
  },
  {
    "objectID": "reference/types.TokenUsage.html",
    "href": "reference/types.TokenUsage.html",
    "title": "types.TokenUsage",
    "section": "",
    "text": "types.TokenUsage\ntypes.TokenUsage()\nToken usage for a given provider (name).",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.TokenUsage"
    ]
  },
  {
    "objectID": "reference/types.ContentJson.html",
    "href": "reference/types.ContentJson.html",
    "title": "types.ContentJson",
    "section": "",
    "text": "types.ContentJson\ntypes.ContentJson(self, value)",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentJson"
    ]
  },
  {
    "objectID": "reference/image_plot.html",
    "href": "reference/image_plot.html",
    "title": "content_image_plot",
    "section": "",
    "text": "content_image_plot(width=768, height=768, dpi=72)\nEncode the current matplotlib plot as an image for chat input.\nThis function captures the current matplotlib plot, resizes it to the specified dimensions, and prepares it for chat input.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)"
  },
  {
    "objectID": "reference/image_plot.html#parameters",
    "href": "reference/image_plot.html#parameters",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nwidth\nint\nThe desired width of the output image in pixels.\n768\n\n\nheight\nint\nThe desired height of the output image in pixels.\n768\n\n\ndpi\nint\nThe DPI (dots per inch) of the output image.\n72"
  },
  {
    "objectID": "reference/image_plot.html#returns",
    "href": "reference/image_plot.html#returns",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object."
  },
  {
    "objectID": "reference/image_plot.html#raises",
    "href": "reference/image_plot.html#raises",
    "title": "content_image_plot",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf width or height is not a positive integer."
  },
  {
    "objectID": "reference/image_plot.html#examples",
    "href": "reference/image_plot.html#examples",
    "title": "content_image_plot",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_plot\nimport matplotlib.pyplot as plt\n\nplt.scatter(faithful[\"eruptions\"], faithful[\"waiting\"])\nchat = ChatOpenAI()\nchat.chat(\n    \"Describe this plot in one paragraph, as suitable for inclusion in \"\n    \"alt-text. You should briefly describe the plot type, the axes, and \"\n    \"2-5 major visual patterns.\",\n    content_image_plot(),\n)"
  },
  {
    "objectID": "reference/types.ContentImageInline.html",
    "href": "reference/types.ContentImageInline.html",
    "title": "types.ContentImageInline",
    "section": "",
    "text": "types.ContentImageInline\ntypes.ContentImageInline(self, content_type, data=None)",
    "crumbs": [
      "Reference",
      "User-facing types",
      "types.ContentImageInline"
    ]
  },
  {
    "objectID": "reference/content_image_url.html",
    "href": "reference/content_image_url.html",
    "title": "content_image_url",
    "section": "",
    "text": "content_image_url(url, detail='auto')\nEncode image content from a URL for chat input.\nThis function is used to prepare image URLs for input to the chatbot. It can handle both regular URLs and data URLs.\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.\n\n\n\n\n\n\nfrom chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#parameters",
    "href": "reference/content_image_url.html#parameters",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nurl\nstr\nThe URL of the image to include in the chat input. Can be a data: URL or a regular URL.\nrequired\n\n\ndetail\nLiteral['auto', 'low', 'high']\nThe detail setting for this image. Can be \"auto\", \"low\", or \"high\".\n'auto'",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#returns",
    "href": "reference/content_image_url.html#returns",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\n[](~chatlas.types.Content)\nContent suitable for a Turn object.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#examples",
    "href": "reference/content_image_url.html#examples",
    "title": "content_image_url",
    "section": "",
    "text": "from chatlas import ChatOpenAI, content_image_url\n\nchat = ChatOpenAI()\nchat.chat(\n    \"What do you see in this image?\",\n    content_image_url(\"https://www.python.org/static/img/python-logo.png\"),\n)",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/content_image_url.html#raises",
    "href": "reference/content_image_url.html#raises",
    "title": "content_image_url",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nValueError\nIf the URL is not valid or the detail setting is invalid.",
    "crumbs": [
      "Reference",
      "Image input",
      "content_image_url"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html",
    "href": "reference/ChatGoogle.html",
    "title": "ChatGoogle",
    "section": "",
    "text": "ChatGoogle(\n    system_prompt=None,\n    turns=None,\n    model=None,\n    api_key=None,\n    kwargs=None,\n)\nChat with a Google Gemini model.\n\n\n\n\n\n\n\n\nAPI key\n\n\n\nTo use Google’s models (i.e., Gemini), you’ll need to sign up for an account and get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGoogle requires the google-generativeai package (e.g., pip install google-generativeai).\n\n\n\n\n\nimport os\nfrom chatlas import ChatGoogle\n\nchat = ChatGoogle(api_key=os.getenv(\"GOOGLE_API_KEY\"))\nchat.chat(\"What is the capital of France?\")\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GOOGLE_API_KEY environment variable.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the genai.GenerativeModel constructor.\nNone\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.\n\n\n\n\n\n\nChatGoogle currently doesn’t work with streaming tools.\n\n\n\nPasting an API key into a chat constructor (e.g., ChatGoogle(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGOOGLE_API_KEY=...\nfrom chatlas import ChatGoogle\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGoogle()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GOOGLE_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#prerequisites",
    "href": "reference/ChatGoogle.html#prerequisites",
    "title": "ChatGoogle",
    "section": "",
    "text": "API key\n\n\n\nTo use Google’s models (i.e., Gemini), you’ll need to sign up for an account and get an API key.\n\n\n\n\n\n\n\n\nPython requirements\n\n\n\nChatGoogle requires the google-generativeai package (e.g., pip install google-generativeai).",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#examples",
    "href": "reference/ChatGoogle.html#examples",
    "title": "ChatGoogle",
    "section": "",
    "text": "import os\nfrom chatlas import ChatGoogle\n\nchat = ChatGoogle(api_key=os.getenv(\"GOOGLE_API_KEY\"))\nchat.chat(\"What is the capital of France?\")",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#parameters",
    "href": "reference/ChatGoogle.html#parameters",
    "title": "ChatGoogle",
    "section": "",
    "text": "Name\nType\nDescription\nDefault\n\n\n\n\nsystem_prompt\nOptional[str]\nA system prompt to set the behavior of the assistant.\nNone\n\n\nturns\nOptional[list[Turn]]\nA list of turns to start the chat with (i.e., continuing a previous conversation). If not provided, the conversation begins from scratch. Do not provide non-None values for both turns and system_prompt. Each message in the list should be a dictionary with at least role (usually system, user, or assistant, but tool is also possible). Normally there is also a content field, which is a string.\nNone\n\n\nmodel\nOptional[str]\nThe model to use for the chat. The default, None, will pick a reasonable default, and warn you about it. We strongly recommend explicitly choosing a model for all but the most casual use.\nNone\n\n\napi_key\nOptional[str]\nThe API key to use for authentication. You generally should not supply this directly, but instead set the GOOGLE_API_KEY environment variable.\nNone\n\n\nkwargs\nOptional['ChatClientArgs']\nAdditional arguments to pass to the genai.GenerativeModel constructor.\nNone",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#returns",
    "href": "reference/ChatGoogle.html#returns",
    "title": "ChatGoogle",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\n\nChat\nA Chat object.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#limitations",
    "href": "reference/ChatGoogle.html#limitations",
    "title": "ChatGoogle",
    "section": "",
    "text": "ChatGoogle currently doesn’t work with streaming tools.",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  },
  {
    "objectID": "reference/ChatGoogle.html#note",
    "href": "reference/ChatGoogle.html#note",
    "title": "ChatGoogle",
    "section": "",
    "text": "Pasting an API key into a chat constructor (e.g., ChatGoogle(api_key=\"...\")) is the simplest way to get started, and is fine for interactive use, but is problematic for code that may be shared with others.\nInstead, consider using environment variables or a configuration file to manage your credentials. One popular way to manage credentials is to use a .env file to store your credentials, and then use the python-dotenv package to load them into your environment.\npip install python-dotenv\n# .env\nGOOGLE_API_KEY=...\nfrom chatlas import ChatGoogle\nfrom dotenv import load_dotenv\n\nload_dotenv()\nchat = ChatGoogle()\nchat.console()\nAnother, more general, solution is to load your environment variables into the shell before starting Python (maybe in a .bashrc, .zshrc, etc. file):\nexport GOOGLE_API_KEY=...",
    "crumbs": [
      "Reference",
      "Chat model providers",
      "ChatGoogle"
    ]
  }
]